---
title: CVPR 2024 RGCA

date: 2024-10-27 17:00:00 +0800

categories: [3D computer vision, Face, papers]

tags: [3D computer vision, Face, papers]

math: true

mermaid: true

description: Relightable Gaussian Codec Avatars
---

# Relightable Gaussian Codec Avatars

![image-20241028221312615](/Users/apple/Documents/GitHub/caimingshuo.github.io/imgs/3dv/3dv9/RGCA_show.png)

>可重新照明的高斯编解码器头像。我们的方法能够使用3D高斯和可学习的辐射转移，通过全频反射和详细的头发重建来实时重新照亮人类头部头像。我们的动态头像可以从head mounted cameras（HMC）拍摄的图像中实时驱动。

### Abstract



### contributions

* 
* 
* 

### PRELIMINARY



### METHOD

![image-20241028222826684](/Users/apple/Documents/GitHub/caimingshuo.github.io/imgs/3dv/3dv9/RGCA.png)

>给定表情的隐变量$z$、gaze$e_{l，r}$和视角$\cal w_{o}$，我们的模型解码3D高斯参数（旋转$\bf R_{k}$、平移$\bf t_{k}$、大小$\bf s_{k}$和不透明度$\bf o_{k}$）和可学习的辐射传递函数（彩色和单色漫反射SH系数$\mathbf{d}_k^c,\mathbf{d}_k^m$、粗糙度$\sigma_{k}$、法向量$\bf{n}_k$和可见度$v_{k}$）。我们将辐射传递函数与输入光集成，以计算最终颜色$\bf c_{k}$，然后我们通过3dgs进行渲染并在图像空间中进行监督。粗顶点解码器$\cal D_{v}$和几何解码器$\cal D_{g}$,外观解码器$\cal D_{ci,cv}$。3.3，以及眼球解码器$\cal D_{ei,ev}$。

#### Geometry: 3D Gaussian Avatars

我们几何表示的核心是3D各向异性高斯的混合，它支持表示不同的拓扑，并且可以表示薄/细的体积结构。这里重点介绍实现可动画化身建模的要点:

与专注于静态场景重建的3DGS相反，我们的目标是构建一个可动画的3D化身表示，它可以跨越一个人的动态面部表情，并且还可以在新颖的照明下重现。这需要一个可重复的化身模型来根据**环境照明的函数**重新着色所有高斯球的$\bf c_{k}$，从而允许在不同的照明条件下真实地适应化身的外观。此外，必须根据任何面部表情的状态注册所有高斯的几何{$\bf g_{k}$}，以确保化身的表达与用户的实际面部动作保持一致。启用任何面部运动的编码和解码对于动画和驱动化身至关重要。

为此，我们在**粗模板网格的共享 UV 纹理图上**参数化3D高斯，并使用2D卷积神经网络对其变换和不透明度进行解码。由于面部表情是高度非线性且难以精确定义的，因此参考之前的工作，我们采用条件变分自动编码器（CVAE）来从数据中学习面部表情的潜在分布。给定以头部为中心的坐标中两只眼睛的眼睛凝视方向$\mathbf{e}_{\{l,r\}}\in\mathbb{R}^3$、简单拟合的网格顶点$\bf V$、展开的平均纹理$\bf T$、我们的编码器$\cal E$和几何解码器$\cal D_{g}$被定义为：
$$
\mu_e,\sigma_e=\mathcal{E}(\mathbf{V},\mathbf{T};\Theta_e),\\\{\delta\mathbf{t}_k,\mathbf{R}_k,\mathbf{s}_k,o_k\}_{k=1}^M=\mathcal{D}_g(\mathbf{z},\mathbf{e}_{\{l,r\}};\Theta_g),
$$

其中$\Theta_{e}$和$\Theta_{g}$别是编码器和解码器的可学习参数，$M$是高斯球的总数，$\sigma_{e}$和$\mu_{e}$是正态分布$\mathbf{z}\sim\mathcal{N}(\mu_e,\sigma_e)$的平均值和标准差。 采样的潜在向量$\mathbf{z}\in\mathbb{R}^{256}$是使用 Kingma 和 Welling [26] 提出的重新参数化技巧计算的。 我们还从$\bf Z$解码网格顶点，以便我们可以通过headset或潜在空间操作为化身设置动画：
$$
\mathbf{V}^{\prime}=\mathcal{D}_v(\mathbf{z};\Theta_v).
$$
请注意，虽然我们直接推断旋转$\bf R$和缩放 $\bf S$，但我们使用粗略几何$g$作为指导，以避免在大运动下出现不良的局部最小值。 最终高斯位置$t_{k}$计算为$\mathbf{t}_{k} = \hat{\mathbf{t}}_{k} + \delta\mathbf{t}_{k}$，其中$\hat{\mathbf{t}}_{k}$是使用$\mathbf{V}^{\prime}$中顶点的重心插值得到的相应 UV 坐标的插值粗网格位置。 为了将 UV 坐标分配给高斯球，我们将一个高斯映射到 UV 贴图中的每个纹理元素。

#### Appearance: Learnable Radiance Transfer

面部外观模型必须准确地模拟各种光传输效果，包括皮肤中的次表面散射、皮肤、眼睛上的镜面反射以及头发上的多次反射散射。 正如早期工作[70, 75] 中所讨论的，虽然漫反射传输算子是入射照明的低通滤波，但镜面传输算子需要能够表示镜面反射的所有频率信息。 为了有效地将网络的容量分配给每个组件，我们将每个 3D 高斯的最终颜色 ck 分解为与视图无关的漫反射项 cdiffuse k 和与视图相关的镜面反射项 cspecular k (ωo)，使得 ck = cdiffuse  k + cspecular k (ωo)，其中 ωo 是观察方向。

