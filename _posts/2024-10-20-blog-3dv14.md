---
title: ECCV 2024 EmoTalk

date: 2024-10-20 17:00:00 +0800

categories: [3D computer vision, Face, papers]

tags: [3D computer vision, Face, papers]

math: true

mermaid: true

description: a paper about talking head

---

## EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking Head

![image-20241024204029844](D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\EmoTalk.png)

### Abstract

我们提出了一种合成**具有可控情绪**的3DTalking heads的新颖方法，具有增强的**唇形同步和渲染质量**。 尽管该领域已经有很多工作取得了重大进展(太卷了😵)，但先前的方法仍然存在**多视图一致性**和**缺乏情感表达**的问题。 为了解决这些问题，我们收集了**带有校准的多视图视频、情感标注和每帧 3D 几何的 EmoTalk3D 数据集**。 通过在 EmoTalk3D 数据集上进行训练，我们提出了一个“**从语音到几何到外观**”的映射框架，该框架首先根据音频特征预测可信的 3D 几何序列，然后预测的几何中合成由4D高斯表示的3DTalking heads的外观。 得到的外观从多视角数据集中学习，可以被进一步解耦为规范的和动态的高斯球，并可以进行融合以渲染自由视图的Talking head动画。 此外，我们的模型支持在生成的Talking head中实现可控的情绪，并且可以在大范围的视图中渲染。 我们的方法在**捕捉动态面部细节**（例如皱纹和微妙表情）的同时，提高了**嘴唇运动生成**的渲染质量和稳定性。 实验证明了我们的方法在生成高保真且情绪可控的 3D Talking head上的有效性。

### Contribution

1. 我们建立了 **EmoTalk3D 数据集**，这是一个带有情感注释的多视图头部说话数据集，具有逐帧的 3D 面部形状。 基于这个**前所未有**的数据集，我们提出了第一个显式情绪可控的 3D 头部说话合成方法。
2. 引入“从语音到几何到外观”的映射框架，以增强 3D 头部说话的口型同步和3D Talking head的整体渲染质量。
3. 提出了**4D 高斯模型**来表示 3D 说话头部的外观，有效地合成动态面部细节，例如皱纹和细微的表情。

### DataSet

![image-20241024214845274](D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\EmoTalk_dataset.png)

​		我们提出了 EmoTalk3D 数据集，这是一个带有**情感注释**的**多视图人脸数据集**，具有重建的 4D 人脸模型和精确的相机校准。 该数据集包含 30 个受试者，每个受试者的数据包含 8 种特定情绪和两种情绪强度下的 20 个sentences，每个受试者的总时长约为 20 分钟。 收集的情绪包括“中性”、“愤怒”、“蔑视”、“厌恶”、“恐惧”、“快乐”、“悲伤”和“惊讶”。 除了“中性”之外，每种情绪都有两种强度——“温和”和“强烈”。 我们邀请了专业的表演导师来指导受试者表达正确的情绪。

​		为了采集数据，我们在与受试者头部相同的水平面**上建造了一个带有 11 个摄像机的圆顶**，并以 180° 度均匀地围绕头部，聚焦于正面。 所有摄像机均与硬件同步系统临时同步，并在捕捉视频之前进行校准。 我们利用了最先进的[多视图 3D 重建算法](https://github.com/jzhangbs/Vis-MVSNet)来重建精确的3D三角形网格模型，然后将其转换为拓扑均匀的3Dmesh模型。每一帧对应的3D表面模型的3D顶点构成**3D点stream**，**即4D点**，用于训练我们的3D说话人脸模型。

​		据我们所知，EmoTalk3D 数据集是第一个包含情感注释的和逐帧的3D 面部几何形状的Talking head数据集。 我们将其与之前的Talking head数据集的参数在下表中进行了比较。该数据集已在我们的项目页面上公开发布用于研究目的。 

<img src="D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\dataset_para.png" alt="image-20241024214917767" style="zoom:50%;" />

### Method

该方法由以下模块组成：

1. 音频编码器，对音频特征进行编码并从输入语音中提取情感标签；  
2. “语音到几何”网络（S2GNet），根据音频特征和情感标签预测动态3D点云；
3. 静态高斯Optimization and Completion模块，用于建立规范的外观模型；
4. “几何到外观网络”（G2ANet),用于合成基于动态3D点云的