---
title: ECCV 2024 EmoTalk

date: 2024-10-20 17:00:00 +0800

categories: [3D computer vision, Face, papers]

tags: [3D computer vision, Face, papers]

math: true

mermaid: true

description: a paper about talking head

---

## EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking Head

![image-20241024204029844](/imgs/3dv/3dv9/EmoTalk.png)

### Abstract

本文提出了一种合成**具有可控情绪**的3DTalking heads的新颖方法，具有增强的**唇形同步和渲染质量**。 尽管该领域已经有很多工作取得了重大进展(太卷了😵)，但先前的方法仍然存在**多视图一致性**和**缺乏情感表达**的问题。 为了解决这些问题，我们收集了**带有校准的多视图视频、情感标注和每帧 3D 几何的 EmoTalk3D 数据集**。 通过在 EmoTalk3D 数据集上进行训练，我们提出了一个“**从语音到几何到外观**”的映射框架，该框架首先根据音频特征预测可信的 3D 几何序列，然后预测的几何中合成由4D高斯表示的3DTalking heads的外观。 得到的外观从多视角数据集中学习，可以被进一步解耦为规范的和动态的高斯球，并可以进行融合以渲染自由视图的Talking head动画。 此外，我们的模型支持在生成的Talking head中实现可控的情绪，并且可以在大范围的视图中渲染。 我们的方法在**捕捉动态面部细节**（例如皱纹和微妙表情）的同时，提高了**嘴唇运动生成**的渲染质量和稳定性。 实验证明了我们的方法在生成高保真且情绪可控的 3D Talking head上的有效性。

### Contribution

1. 我们建立了 **EmoTalk3D 数据集**，这是一个带有情感注释的多视图头部说话数据集，具有逐帧的 3D 面部形状。 基于这个**前所未有**的数据集，我们提出了第一个显式情绪可控的 3D 头部说话合成方法。
2. 引入“从语音到几何到外观”的映射框架，以增强 3D 头部说话的口型同步和3D Talking head的整体渲染质量。
3. 提出了**4D 高斯模型**来表示 3D 说话头部的外观，有效地合成动态面部细节，例如皱纹和细微的表情。

### DataSet

![image-20241024214845274](/imgs/3dv/3dv9/EmoTalk_dataset.png)

​		我们提出了 EmoTalk3D 数据集，这是一个带有**情感注释**的**多视图人脸数据集**，具有重建的 4D 人脸模型和精确的相机校准。 该数据集包含 30 个受试者，每个受试者的数据包含 8 种特定情绪和两种情绪强度下的 20 个sentences，每个受试者的总时长约为 20 分钟。 收集的情绪包括“中性”、“愤怒”、“蔑视”、“厌恶”、“恐惧”、“快乐”、“悲伤”和“惊讶”。 除了“中性”之外，每种情绪都有两种强度——“温和”和“强烈”。 我们邀请了专业的表演导师来指导受试者表达正确的情绪。

​		为了采集数据，我们在与受试者头部相同的水平面**上建造了一个带有 11 个摄像机的圆顶**，并以 180° 度均匀地围绕头部，聚焦于正面。 所有摄像机均与硬件同步系统临时同步，并在捕捉视频之前进行校准。 我们利用了最先进的[多视图 3D 重建算法](https://github.com/jzhangbs/Vis-MVSNet)来重建精确的3D三角形网格模型，然后将其转换为拓扑均匀的3Dmesh模型。每一帧对应的3D表面模型的3D顶点构成**3D点stream**，**即4D点**，用于训练我们的3D说话人脸模型。

​		据我们所知，EmoTalk3D 数据集是第一个包含情感注释的和逐帧的3D 面部几何形状的Talking head数据集。 我们将其与之前的Talking head数据集的参数在下表中进行了比较。该数据集已在我们的项目页面上公开发布用于研究目的。 

<img src="/imgs/3dv/3dv9/dataset_para.png" alt="image-20241024214917767" style="zoom:50%;" />

### Method

该方法由以下模块组成：

1. 音频编码器，对音频特征进行编码并从输入语音中提取情感标签；  
2. “语音到几何”网络（S2GNet），根据音频特征和情感标签预测动态3D点云；
3. 静态高斯Optimization and Completion模块，用于建立规范的外观模型；
4. “几何到外观网络”（G2ANet),用于合成基于动态3D点云的面部外观；

上述模块共同构成了用于情感Talking head合成的“从语音到几何到外观”映射框架。

#### Speech-to-Geometry

![image-20241025154800589](/imgs/3dv/3dv9/pipline of emoTalk.png)

如上图所示，“语音到几何”模块由两部分组成：将**输入语音转换为音频特征**的音频编码器和**将音频特征和情感标签转换为3Dmesh序列**的S2GNet。 具体来说就是，使用了预训练的[HuBERT](https://arxiv.org/abs/2106.07447)语音模型作为音频编码器。 以及受到Baevski等人工作的启发，我们采用**遵循Transformer架构的上下文网络**(在图中没有标出来😵？)作为我们的情感提取器的支柱。并且该模型使用了Baevski等人工作中的预训练权重进行初始化，然后使用数据集中的真实情感标签和语音音频进行微调。

​		S2GNet 的设计遵循了FaceXHubert，这是一种最先进的audio-to-mesh的预测网络。 具体来说,S2GNet接收**提取的情感标签和编码的音频特征**作为输入，基于平均模板网格回归顶点位移，最终生成4D的Talking mesh序列。值得一提的是，S2GNet不是那种常见的基于transformer架构的网络，其选择了门控循环单元GRU作为核心的架构。在实验中，我们已经验证了这种方法在口型同步和语音泛化方面的卓越性能。

​		我们利用FaceXHubert的预训练模型来初始化S2GNet的训练，然后在EmoTalk3D数据集上微调 S2GNet。  S2GNet 预测的三角形网格模型会经过四倍上采样，然后在用作“**几何到外观**”模块的输入。

#### Geometry-to-Appearance

在预测4D面部点后，我们引入了Geometry2Appearance Network（G2ANet）以4D的面部点作为输入来合成说话人脸的3D高斯特征，如上图所示。

​		G2ANet 的设计基于两个关键观察结果。 首先，我们认识到了对于特定个体的Talking head，**面部运动会产生相对较小的外观变化**。 考虑到这一点，**我们最初在静态、不说话的头部上训练高斯模型**，称为**规范高斯模型**。 随后，**使用规范高斯和4D点导出的面部运动作为输入，采用神经网络来预测因运动而引起的外观变化**。 这些预测的变化被称为动态高斯。 其次，鉴于语音与头发、颈部和肩膀等非面部特征之间没有直接关联，S2GNet 网络**忽略预测**这些元素的几何结构。 因此，G2ANet 学习复制这些非脸部的部分的外观，并将它们与脸部无缝集成起来。

* **Canonical 3D Gaussians**

在我们的方法中，规范的3D Gaussians $G_{o}$代表静态头部头像，并且是从无语音时刻的多视图图像（通常是视频剪辑的第一帧）中学习的。 规范的3D高斯球$G_{o}$表示为：


$$
\mathbf{G}_{o}\leftarrow\{\mu_{o},\mathbf{S}_{o},\mathbf{R}_{o},\alpha_{o},\mathbf{H}_{o}\}
$$

* **Dynamic Detail Synthesis**

为了使用3D高斯球来生成3D的Talking face，我们使用**动态细节**来预测$t$时刻的外观。 动态细节是指由于Talking中的面部运动而产生的细节的外表，例如特定的皱纹和轻微的表情。 对于特定的某个主体，不透明度$\alpha$和比例$\mathbf{S}$保持不变，分别等于$\alpha_{o}$和$\bf{S}_{o}$。在$t$时刻的高斯球$G_{t}$可以被公式化为：


$$
\mathbf{G}_{t}\leftarrow\{\mu_{t},\mathbf{S}_{o},\mathbf{R}_{t},\alpha_{o},\mathbf{H}_{t}\}
$$
由于$\mu_{t}$已在"语音到几何"网络中进行了预测，因此只有$\mathbf{H}_{t}$和$\mathbf{R}_{t}$是未知的，分别由$FeatureNet$（4层的MLP）和$RotationNet$（两层的MLP）进行预测：


$$
\mathbf{H}_{t}=FeatureNet(\mathbf{H}_{o},e,p,\delta\mu)
$$

$$
\begin{aligned}\mathbf{R}_t=RotationNet(\mathbf{R}_o,e,p,\delta\mu)\end{aligned}
$$

其中$e$是情感向量,$p$是为每个点预定义的在UV坐标中的位置，$\delta\mu$定义为：


$$
\delta\mu=\mu_t-\mu_o-\frac{1}{N}\sum_{i\in G}(\mu_t^i-\mu_o^i)
$$
通过实验，我们发现将**规范高斯**与**动态高斯**相结合可以预测高度详细的动态面部外观。 这些包括愤怒或其他表情时形成的皱纹以及嘴唇运动引起的细微外观变化，在此之前没有一个头部说话模型能够如此细致地呈现动态面部细节。

#### Head Completion

语音到几何网络仅预测面部的3D 点云，因为面部以外的几何形状（例如头发、颈部和肩膀）与语音信号没有强相关性 。因此，这些人脸以外的区域没有精确的初始点来优化高斯，因此我们通过**3DGS优化**(好奇怎么做到的😋)在空间中添加**85、500个均匀分布的点**来构成人脸以外的区域，如下图所示。

![image-20241025163333251](/imgs/3dv/3dv9/EmoTalk2.png)

面部区域以外的点（简称OTF点），包括头发、颈部和肩部，应该在一定程度上跟随面部点运动。 我们观察到，这些点在远离面部的点（例如肩膀）往往是静止的，而靠近面部的点会一起移动，例如头发和颈部。 因此，我们将除面部点$\delta\mu_{otf}$之外的点的运动表示为：



$$
\begin{aligned}\delta\mu_{otf}=\delta\mu_f\cdot e^{-\alpha d}\end{aligned}
$$


其中$\delta\mu_{f}$是最**接近该OTF点的面部点**的运动；  $r$是OTF点与其最近的面部点之间的距离;$\alpha$是衰减因子，在我们所有的实验中均设置为 0.1。

​		在动态高斯的训练中，我们观察到，以与**面部相同**的方式训练头发和肩膀上的高斯会导致**严重的模糊**。 这是因为我们的数据集中的面部点位置被准确恢复，因此这些高斯点可以根据视频进行逐帧的准确对齐。 相比之下，面部外部区域（包括头发、颈部和肩膀）的外观是从随机点进行优化的，这些 OTF 点的 3D 位置不可靠，因此由于这些点对应于不同帧的未对齐外观，导致预测的外观十分模糊。 另一方面，规范高斯球所代表的完整头部是清晰的；然而，规范高斯的外观是静态的。

​		为了解决这个问题，我们分别处理动态部分（脸部）和静态部分（除了脸部以外的肩膀上方的区域），前者使用动态高斯，后者使用规范高斯。 具体地，我们预先定义了**一个自然过渡的面部权重掩码**，将其应用于不透明度$\alpha$以实现动态和静态部分的自然融合。 点$p$的权重$W_{p}$可以被公式化为：



$$
W_p=\left\{\begin{matrix}0,&\quad d<d_0\\\frac{d-d_0}{d_{th}},&\quad d_0\leq d\leq d_0+d_{th}\\1,&\quad d_0+d_{th}<d\end{matrix}\right.
$$


其中$d$是点$p$与其最近的面部点之间的距离；$d_{0}$和$d_{th}$是过渡因子，分别设置为$5mm$和$10mm$。 规范高斯和动态高斯的不透明度场分别乘以$W_{p}$和$1-W_{p}$ 。这样，说话的面部外观和其他外观（头发、颈部和肩膀）融合在一起，合成一个清晰完整的3D说话头部。



>```
>@inproceedings{he2024emotalk3d,
>  title={EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking Head},
>  author={He, Qianyun and Ji, Xinya and Gong, Yicheng and Lu, Yuanxun and Diao, Zhengyu and Huang, Linjia and Yao, Yao and Zhu, Siyu and Ma, Zhan and Xu, Songchen and Wu, Xiaofei and Zhang, Zixiao and Cao, Xun and Zhu, Hao},
>  booktitle={European Conference on Computer Vision (ECCV)},
>  year={2024}      
>}
>```













