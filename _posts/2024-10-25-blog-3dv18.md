---
title: CVPR 2024 SurFHead

date: 2024-10-22 17:00:00 +0800

categories: [3D computer vision, Face, papers]

tags: [3D computer vision, Face, papers]

math: true

mermaid: true

description: SurFHead
---

# DiffusionAvatars: Deferred Diffusion for High-fidelity 3D Head Avatars

![image-20241027104635334](/Users/apple/Documents/GitHub/caimingshuo.github.io/imgs/3dv/3dv9/DiffusionAvtar1.png)

> 给定一组多视图视频和相应的匹配mesh，我们构建一个人的扩散化身。我们的方法将可变形模型的表情转化为人的真实面部外观，同时还提供对视角的控制。

### Abstract

DiffusionAvatars合成了一个高保真的3D头像，可以直观地控制姿势和表情。我们提出了一种基于扩散的神经渲染器，它利用一般的2D先验知识来产生**人脸的压缩图像**。对于表情和头部姿势的粗略引导，我们从目标视点绘制了一个神经参数头部模型(NPHM)，作为人的代理几何。此外，为了增强对复杂面部表情的建模，我们通过**交叉注意**直接根据从NPHM获得的**表情代码$exp_{code}$**来**条件性控制**DiffusionAvatars。最后，为了合成不同视角和表情之间一致的表面细节，我们在**NPHM的规范空间**中通过**三平面查找**将**可学习的空间特征**装配到头部表面。我们在RGB视频和对应的人的NPHM网格上训练DiffusionAvatars，并在*self-reenactment*和动画场景中测试获得的化身。我们的实验表明，DiffusionAvatars为一个人的新姿势和表情生成了时间上一致和视觉上有吸引力的视频，性能优于现有的方法。

### contributions

* DiffusionAvatars是一个基于扩散模型的神经渲染器，它利用ControlNet来创建可动画的3D头部化身。
* 我们设计了一种方法，通过TriPlanes将可学习的空间特征装配到底层NPHM的表面上。
* 我们提出通过交叉注意力进行直接表情$conditioning$，将详细表情从NPHM转移到合成的3D头部化身。

### Method

我们的目标是创建一个时间一致的3D头像，一个人的头像，可以明确地控制视角和表情。我们通过设计一个基于扩散模型的神经渲染器来实现这一任务，该渲染器对隐式定义的代理几何表面上操纵的可学习特征进行解码。类似于延迟神经渲染，我们将这种方法称为**延迟扩散**。我们首先介绍扩散模型的基础和NPHM，在详细介绍我们的方法之前，我们将其用作代理的几何：在光栅化和表面特征映射阶段，我们**为基于扩散的神经渲染器生成输入图像**。这些渲染编码了所需的视角、头部形状和全局表情。为了更细粒度的表达控制，我们将NPHM的表情代码作为额外的输入添加到扩散模型中。最后，我们的pipline利用预先训练的扩散模型的图像合成和泛化能力，以获得更好的质量。

### Diffusion

我们的工作建立在操纵自编码器的隐变量空间的latent扩散模型（LDM）之上；在形式上，LDM使用编码器$\cal E$将RGB图像映射到低维潜在空间，从而促进生成任务。在下文中，任何提到图像$x_{0}$的地方总是指的是将RGB图像$I$映射到自编码器的隐空间中来获得隐图像：$x_0=\cal{E}(I)$

在扩散中，固定的正向过程中迭代地向隐图像添加噪音是这样：
$$
q(x_\tau|x_{\tau-1})=\mathcal{N}\left(x_\tau;\sqrt{\alpha_\tau}x_{\tau-1},(1-\alpha_\tau)\mathbf{I}\right)
$$
其中$\tau=1...N$，表示去噪的step,前向过程方差$\alpha_{\tau}$代表对噪声的控制，实际上，噪声样本$x_{\tau}$是通过标准高斯重新参数化获得的：
$$
x_\tau=\sqrt{\bar{\alpha}_\tau}x_0+\sqrt{1-\bar{\alpha}_\tau}\epsilon\quad\epsilon\sim\mathcal{N}\left(0,\mathbf{I}\right)
$$
通常，扩散模型被训练来预测给定噪声样本$x_{\tau}$的原始噪声$\cal E$。但我们找到了一种被称为[v参数化](https://arxiv.org/abs/2202.00512)的不同方法，在我们的场景中效果更好，其中v被定义为：
$$
v=\sqrt{\bar{\alpha}_\tau}\epsilon-\sqrt{1-\bar{\alpha}_\tau}x_0
$$
这样做有两个好处：首先，v-预测确保损失始终可以引导模型学习一些有意义的东西和更快的收敛，即使输入已经包含了大量噪音。其次，它允许我们还在纯噪生的输入上训练模型，这在推理期间是最具挑战性的去噪步骤。因此，我们重新调整了[噪声时间表](https://arxiv.org/abs/2305.08891)$\alpha$，以确保训练期间的**零信噪比输入**。

### NPHM

NPHM是一种可变形的参数化头部模型，它在给定身份代码$z_{id}$和表情代码$z_{exp}$的情况下生成人头的SDF函数。我们使用COLMAP来获得一个人体的多视角视频的每个时间步t的点云${P_{t}}$。随后，我们将NPHM匹配到每个点云${P_{t}}$以获得$z_{id}$和$z_{exp}$。我们使用了NPHM的变体MonoN-PHM，它使用后向的变形场而不是最初提出的向前变形场，因为它简化了拟合的过程。形式上，使用NPHM拟合如下：
$$
z_{id},z_{exp}^t=\arg\min_{z_{id},z_{exp}}\sum_{x\in\mathcal{P}_t}|\mathcal{F}_{id}(\mathcal{F}_{exp}(x))|
$$
其中$F_{id}$是NPHM的身份网络(实际上是一个SDF函数)，$F_{exp}$是后向的变形场网络。为了简单起见，我们放弃了$F_{id}$和$F_{exp}$对其各自latentcode的依赖性(~~?~~),拟合得到的SDF表征可以通过march cubes转化为一个mesn$M_{t}$,此外，对于提取到的网格的每个顶点$x\in M_t$，我们可以通过后向的变形场获得其规范坐标$x_{can}\in\mathbb{R}^{3+2}$：
$$
x_{can}=\mathcal{F}_{exp}(x)
$$
其中$x_{can}$的前3个坐标代表正常的空间维度坐标，而其余2个坐标是可以帮助解决嘴巴和眼睛区域中一些拓扑问题的空间维度。在随后的光栅化阶段中，我们会利用网格$M_{t}$，其中规范坐标$x_{can}$对于空间特征的查找是必要的。

### Rasterization

我们使用nvdiffrast来生成扩散模型的实际输入图像。令$M_{t}$是一个人在相机位姿$\pi_{t}$和时间步$t$下的的面部mesh。然后使用光栅化器$R$来获得这个时间步的一组渲染结果$R^t\in\mathbb{R}^{H\times W\times C}$：
$$
R^t=\mathcal{R}\left(M_t,\pi_t\right)
$$
在我们的情况中，$R^{t}$的通道是















>```
>@article{qian2023gaussianavatars,
>  title={GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians},
>  author={Qian, Shenhan and Kirschstein, Tobias and Schoneveld, Liam and Davoli, Davide and Giebenhain, Simon and Nie\ss{}ner, Matthias},
>  journal={arXiv preprint arXiv:2312.02069},
>  year={2023}
>}
>```























