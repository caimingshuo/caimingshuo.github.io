---
title: Point-based-head-share

date: 2024-09-21 17:00:00 +0800

categories: [3D computer vision, Face, papers]

tags: [3D computer vision, Face, papers]

math: true

mermaid: true

description: Some papers about Point-based-head-share

---



## PointAvatar: Deformable Point-based Head Avatars from Videos

从随意的视频序列中创建逼真的可动画化和可灯光化头像的能力将在通信和娱乐领域开辟广泛的应用。目前的方法要么建立**在显式三维变形网格(3DMM)**上，要么利用神经隐式表征。前者受固定拓扑结构的限制，而后者易变形且渲染效率低。此外，现有的方法在颜色估计中纠缠光照，因此它们在新环境中重新渲染角色时受到限制。相比之下，我们提出了PointAvatar，这是一种可变形的基于点的表示，它将源颜色分解为内在反照率和法线相关的阴影。我们演示了PointAvatar在现有的网格表示和隐式表示之间架起了桥梁，将高质量的几何形状和外观与拓扑灵活性、易于变形和渲染效率相结合。我们表明，我们的方法能够使用来自多个来源的单目视频(包括手持智能手机，笔记本电脑网络摄像头和互联网视频)生成可动画的3D化身，在以前的方法失败的具有挑战性的情况下(例如，细发丝)实现最先进的质量，同时在训练中显着比竞争方法更有效。

## HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors

![image-20241022162456768](D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\image-20241022162456768.png)

在本文中，我们介绍了一种新颖的三维头像创建方法，这种方法能够从**很少的野外拍摄数据**中生成**高保真**和**可动画化**的三维头像。考虑到这一问题的**低约束性**，结合**先验知识**至关重要。因此，我们提出了一个框架，包括先验学习和头像创建两个阶段。先验学习阶段利用从**大规模多视角动态数据集**中获得的**三维头部先验知识**，而头像创建阶段则应用这些先验知识进行少量拍摄的个性化处理。我们的方法利用**基于3DGS的自动解码器网络**和**Part-based**的动态建模，有效地捕捉了这些先验。我们的方法利用身份共享编码和个人身份的个性化潜码来学习高斯点的属性。在头像创建阶段，我们利用反转和微调策略实现了快速的头部头像个性化。广泛的实验证明，我们的模型有效地利用了头部先验，并成功地将其推广到少镜头个性化场景中，实现了逼真的渲染质量、多视角一致性和稳定的动画效果。

### Contributions

* 我们介绍了一种新的框架，利用可推广的3D高斯先验，只使用几个输入图像，就可以快速3D头部头像个性化，。这些化身表现出高保真度和一致的动画质量。
* 我们的设计可以有效地利用part-based动态高斯头部先验，并将其推广为高品质的少样本头部个性化人头重建。

### Method

![image-20241022182526650](D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\Pipline)

我们的头像表示基于**自动解码器先验模型**，该模型可以从**多个身份**中学习**头部先验知识**，并用于从少数拍摄图像中创建头部头像。如上图所示，我们的表示建立在**基于点**的表示和Part-based的建模之上，其中**每个点只负责一个语义部分**。首先，我们初始化Part-based的特征点云，包括**1）Part-based的id代码**和2）基于跟踪网格的**特定高斯点的特征编码**。然后，我们进行动态建模，通过将特征点云输入Part-based的多层感知（MLP）中去回归得到用于渲染的高斯点的属性。最后，我们利用**卷积神经网络（CNN）模块来细化3DGS渲染**，以获得最终渲染图像。在下文中，我们将描述这些关键的组件。

#### Avatar Representation

* **Part-based Feature Point Cloud**

为了初始化基于tracked mesh$M$的Part-based Feature Point Cloud，我们首先利用**基于UV的初始化**[[FlashAvatar]]([USTC3DV/FlashAvatar-code: [CVPR 2024\] The official repo for FlashAvatar](https://github.com/USTC3DV/FlashAvatar-code))来获得n个初始的高斯点，同时每个像素都被绑在了mesh平面上的三角形中，与**Face-based**的初始化方式相比，这样的初始化有助于在头部区域上分布更均匀的基元。

然后，我们为点云设置初始特征。特征包含两种类型：

1. 特定点的特征编码$\mathbf{f}=\left\{\mathbf{f}_{i} \in \mathbb{R}^{c_{1}}\right\}_{i=1}^{n}$
2. 部分id编码$\mathbf{z}=\left\{\left\{\mathbf{z}_{j}^{l} \in \mathbb{R}^{c_{2}}\right\}_{l=1}^{p}\right\}_{j=1}^{k}$

其中$p$和$k$分别表示Part和id_number。点编码$\mathbf f$嵌入了id共享的先验并且id码$\mathbf z$用作自动解码器模型的**id_codebook**（~~这是啥~~）。**所有编码都是随机初始化的可学习参数**(参数化)。高斯点的部分由其父三角形确定，属于同一部分的高斯点共享所有的共享相同的id_code。

* **Part-based Dynamic Gaussian Attributes Modeling**

为了简单起见，我们使用$\mathbf f$和$\mathbf z$来表示属于特定部分$p$的逐点的特征。对于给定的$\mathbf f$和$\mathbf z$，通过这样的方式来进行动态局部的高斯属性:
$$
\mathcal{A}^g=f_p^{\mathcal{M}_1}(\mathbf{f},\mathbf{z}),\quad\mathbf{h}=f_p^{\mathcal{M}_2}(\mathbf{f},\mathbf{z},\mathbf{e},\mathcal{A}^g)
$$
其中$\mathcal{A}^g=\{\boldsymbol{\mu}^{\prime},\mathbf{r}^{\prime},\mathbf{S}^{\prime},\alpha\}$是除了位置之外的一个高斯点的局部属性，并且${\bf e}:=\,{\cal D}(\mu^{\prime})\,=\,{\cal T}(\mu^{\prime})\,-\mathcal{T}\left(\boldsymbol{\mu}_{\text {neutral }}^{\prime}\right)$是通过全局位姿点位置${\cal T}(\mu^{\prime})$减去全局中性点位置$$\mathcal{T}\left(\boldsymbol{\mu}_{\text {neutral }}^{\prime}\right)$$获得的点特定动态信号;$f_p^{\mathcal{M}_1}$和$f_p^{\mathcal{M}_2}$都是part-specific的多层神经网络。我们定义整体的动态建模过程为$\mathcal{A}^f=f_p^{\mathcal{M}}(\mathbf{f}, \mathbf{z})$，其中$\mathcal{A}^f=\mathcal{A}^g \cup\{\mathbf{h}\}$表示用于最终用于Splatting过程的高斯点的属性。

Part-based和动态的建模都有助于小样本情况下的表现，Part-based的建模允许特定的模块去学习特定部分的先验，从而更容易优化并获得更强大的先验知识。动态的建模采用**点特定的表达信号**${\bf e}$来**预测动态局部属性**，这比使用静态局部属性的[GaussianAvatars]([[2312.02069\] GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians](https://arxiv.org/abs/2312.02069))在捕捉动态细节方面更好。

* **Gaussian Splatting with CNN refinement**

受到[一些工作]([[2305.01190\] LatentAvatar: Learning Latent Expression Code for Expressive Neural Head Avatar](https://arxiv.org/abs/2305.01190))的启发，作者再次也采用了一个Scree-Space(~~为什么是这个名字~~)的CNN$f^{C}$来refine渲染的结果：
$$
\left[\mathbf{I}_{r g b}, \mathbf{I}_h\right]=\mathcal{R}\left(\mathcal{T}\left(\mathcal{A}_f, \mathcal{M}\right), \pi_{\mathbf{K}, \mathbf{E}}\right)
$$

$$
\mathbf{I}=f^{\mathcal{C}}\left(\left[\mathbf{I}_{r g b}, \mathbf{I}_h\right]\right)
$$

其中$\mathcal{A}^f=\left\{\mathcal{A}_i^f\right\}_{i=1}^n$代表最后所有点的高斯属性，$\bf I_{rgb}$是最终渲染出来的RGB图像，$\bf I_{h}$是用于CNN refinement的潜在特征图像。

与[之前工作]([YuelangX/Gaussian-Head-Avatar: [CVPR 2024\] Official repository for "Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians"](https://github.com/YuelangX/Gaussian-Head-Avatar))不同的是,我们不使用CNN来进行超分，而是保持输入和输出具有相同的分辨率，以进行细化；我们的目标是使用大规模的训练数据，使CNN能够捕捉到泛化的结构化外观先验知识，这对于3DGS-based表征方法是难以做到的，后面的实验分析也证明了，只用CNN来细化，得到的效果确实更加逼真，实现了few-shot的个性化操作。

* **Overall Representation**

至此，整个的头部化身被正式地定义为：
$$
\mathcal{H}:\left(\mathcal{M} ; f^{\mathcal{M}}, f^{\mathcal{C}}, \mathbf{f}, \mathbf{z}\right) \mapsto \mathbf{I}
$$

#### Head Prior Learning

我们高度依赖于头部的先验知识，以只有几个输入图像情况下无约束地实现高保真的化身重建。在各种先验知识中，我们的目标是从能获得的具有多个身份的多视图动态头部数据中学习高质量、可动画化和3D连续的头部先验知识。因此，先验学习阶段的目标是去使用GAPNet用id码$\mathbf{Z}_{1 \ldots k}$和其他优化的网络参数来学习对应的$k$个人头化身，在开始模型训练之前，我们首先对训练数据进行FLAME估计得到$\text{M}$。然后，我们使用这些数据来联合优化$\mathcal{M}, f^{\mathcal{M}},$$f^{\mathcal{C}},\mathrm{f},$和$\bf z$。这是总的损失函数：
$$
\mathcal{L}=\mathcal{L}_{rec}(\mathbf{I},\mathbf{I}^{*})+\mathcal{L}_{rec}(\mathbf{I}_{rgb},\mathbf{I}^{*})+\lambda_{m}\mathcal{L}_{rec}(\mathbf{I}_{m},\mathbf{I}_{m}^{*})+\mathcal{L}_{reg},
$$
其中$\mathcal{L}_{rec}$和$\mathcal{L}_{reg}$分别表示图像重建损失和训练正则化损失,真值图被表示为$\mathbf{I}^{*}$,为了提高嘴部区域的保真度，我们受[[FlashAvatar]]([USTC3DV/FlashAvatar-code: [CVPR 2024\] The official repo for FlashAvatar](https://github.com/USTC3DV/FlashAvatar-code))启发,进一步用的掩蔽的真实嘴部区域$\mathbf{I}_{m}^{*}$来监督掩蔽的渲染嘴部区域$\mathbf{I}_{m}$;具体来说，图像的重建损失为：
$$
\mathcal{L}_{rec}=\lambda_{l1}\mathcal{L}_{l1}+\lambda_{ssim}\mathcal{L}_{ssim}+\lambda_{lpips}\mathcal{L}_{lpips}
$$
与此同时，训练正则化损失为：
$$
\mathcal{L}_{reg}=\lambda_{\alpha}\mathcal{L}_{\alpha}+\lambda_{s}\mathcal{L}_{s}+\lambda_{\mu}\mathcal{L}_{\mu}+\lambda_{arap}\mathcal{L}_{arap},
$$
这其中包括了不透明度正则项$\mathcal{L}_{\alpha}=\|\mathbf{I}_{\alpha}\mathbf{-}\tilde{\mathbf{I}}_{mask}\|_{1}$,原始局部scale正则化$\mathcal{L}_s=\|\max(\mathbf{s},\epsilon_s)\|_2$,原始局部位置正则化$\mathcal{L}_\mu=\|\max(\boldsymbol{\mu},\epsilon_\mu)\|_2$,还有来自[变形领域经典论文]([igl.ethz.ch/projects/ARAP/arap_web.pdf](https://igl.ethz.ch/projects/ARAP/arap_web.pdf))(~~论文我也没看过~~)的ARAP损失$\mathcal{L}_{arap}$，....

#### Few-shot Personalization

在先验学习阶段之后，我们用GAPNet来编码动态头部先验知识;因此所有GAPNet学到的参数都可以作为强大的先验用于few-shot甚至one-shot的个性化人头重建。

在个性化重建之前，我们采用了一个**tracker**(~~这是啥东西，这个要学一下~~)获取输入图像的FLAME参数。给定具FLAME跟踪的输入图像，我们首先通过inversion从id_codebook中找到最相似的化身。具体地说，我们优化部件特定的线性组合权重$\textbf{w}\in\mathbb{R}^{\tilde{k}\times p\times1}$，以获得用于渲染与输入相似的化身的身份码$\mathbf{z}^{*} = \mathrm{softmax}(\mathbf{w})\odot\mathbf{z} \in \mathbb{R}^{k\times p\times c_{2}}$。在反演优化过程中，除了$\mathbf{w}$之外，我们保持网络的所有参数冻结。在形式上，给定目标身份的输入图像$\bf I ^{*}$，我们优化以呈现类似于目标身份的图像$\bf I$。

然后，我们开始微调以更新网络的参数，以便化身可以从输入中捕获目标身份的细节。我们通过**三种策略**在这个过程中利用先验知识。首先，我们对除**特征编码$\bf f$之外**的所有参数使用小的学习率。接下来，我们通过排除嘴部区域的微调过程来充分利用之前提取的Part-based的先验，因为用很少的输入来建模高度灵活的嘴部区域是十分有挑战性的。最后，我们应用视图正则化来防止目标视图的过拟合：具体地，我们将具有中性面$\{\mathbf{R}_i\}_{i=1}^m$的一些参考视图的微调结果约束为接近微调$\{\tilde{\mathbf{R}}_i\}_{i=1}^m$之前的渲染结果，其中$m$是生成的参考视图的数量。利用先验知识，我们的个性化化身实现了稳定的再现，同时保留了目标身份的细节。微调是通过最小化方程中的损失函数来进行的:
$$
\arg\min_\xi\mathcal{L}_f=\mathcal{L}(\mathbf{I},\mathbf{I}^*)+\lambda_{ref}\sum_{i=1}^m(\mathcal{L}(\mathbf{R}_i,\tilde{\mathbf{R}}_i)),
$$
其中$\xi$表示所有可学习的参数，$\lambda_{ref}$用于平衡不同的损失项。

# Generalizable and Animatable Gaussian Head Avatar

* **双提升方法**(Dual-lifting)

3D高斯的重建需要数百个高斯点，这对于从单图中重建是十分要命的，作者提出了一种**双提升**方法：首先通过冻结的DINOv2框架得到局部特征平面$F_{local}$,
$$
G_{pos}=[p_{s}+E_{Conv0}(F_{local})\cdot n_{s},\quad p_ {s}-E_{Conv1}(F_{local})\cdot n_{s}]
$$

$$
G_{c, o, s, r}=\left[E_{\text {Conv0 }}\left(F_{\text {local }}\right), \quad E_{\text {Conv1}}\left(F_{\text {local }}\right)\right]
$$

其中$p_{i}$是基于$I_{s}$的估计相机姿态映射的初始点平面，并且通过原点。$p_{i}$的大小为296 × 296，这与局部特征$F_{local}$一致。$E_{Conv0，1}$是卷积网络，$n_{s}$是$p_{s}$的法向向量，$G_{pos}$是3D高斯点的位置，$G_{c, o, s, r}$表示3D高斯的颜色、不透明度、比例和旋转。

# GGHead: Fast and Generalizable 3D Gaussian Heads

![image-20241023102829509](D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\gghead)

> 我们的方法可以基于3DGS生成不同的3D头部表示。被抽样的人展示了详细的几何形状和外观。生成和渲染都可以以全图像分辨率实时进行而不需要2D超分辨率网络。

#### Abstruct

从大型2D图像集合中学习3D头部先验信息是迈向高质量3D感知人体建模的重要一步。一个核心需求是一个可以很好地扩展到大规模数据集和大图像分辨率的高效的架构。不幸的是，现有的3D GAN由于其**相对较慢的训练和渲染速度**而难以扩展以**生成高分辨率**的样本，并且通常不得**不依赖于要牺牲全局3D一致性的2D超分辨率网络**。为了解决这些挑战，我们提出了GGHead，它在3D GAN框架内采用了最近的3dgs。为了生成3D表示，我们采用了一个强大的2D CNN生成器**来预测UV空间中模板头部mesh的的高斯点属性**。通过这种方式，GGHead利用了**模板UV布局的规律性**，大大促进了**预测非结构化3D高斯集**的挑战性任务。我们在**渲染的UV坐标上用一个新的总体变分损失**进一步提高了生成的3D表示的**几何保真度**。直观地说，这种正则化鼓励相邻的渲染像素应该源于模板的UV空间中的相邻高斯。总而言之，我们的pipeline可以通过在单视角的2D图片上生成训练的3D头部。我们提出的框架与在FFHQ数据集训练的现有的3D头部GAN的质量相匹配，同时速度更快且完全3D一致。因此，我们首次展示了以$1024^2$分辨率实时生成和渲染高质量的3D一致性头部。

#### Contributions

* 我们将3D Gaussian Heads参数化为UV maps(~~所以这个到底要怎么使用呢~~)，这种UV map可以使用高效的2D CNN结合一种新的UV总体变分正则化来生成，从而提高几何保真度。
* 我们的方法具有高度可扩展性，便于在1k分辨率下进行训练，同时实现实时样本生成和头部渲染

#### Method

![image-20241023105009718](D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\gghead_pipline)

> 我们在3D GAN公式中采用3DGS，实现了仅从2D图像中去学习3D头部的分布。为了构建3D高斯表示，我们利用了模板mesh的UV空间。然后CNN生成器**利用正态分布的隐编码**来预测每个高斯属性的2D映射。为了获得用于光栅化的实际3D高斯基元，我们对UV贴图进行均匀采样，并将基元相对于模板网格放置。然后，所生成的3D高斯表示被光栅化并由鉴别器监督。为了提高训练稳定性，特别是在对抗训练的早期阶段，我们通过$\mathcal{L}_{reg}^{pos},\mathcal{L}_{reg}^{scale}$和$\mathcal{L}_{reg}^{Opac}$来调整预测的$pos$、$scale$和$opacity$属性的偏移。此外，我们提出了**一种新的UV总体变分损失**$\mathcal{L}_{uv}$，通过强制UV渲染的平滑性以提高生成的3D头部的几何保真度。

* **3D Gaussian Splatting (3DGS)**

3DGS是一种基于点的场景表示，为每个点分配五个不同的属性:高斯中心$\mu\in\mathbb{R}^3$,大小$\mathbf{s}\in\mathbb{R}^3$,参数化为四元数的旋转表征$\mathbf{q}\in\mathbb{R}^4$,颜色$\mathbf{c}\in\mathbb{R}^3$和密度/不透明度$\sigma\in\mathbb{R}$:
$$
G^i=\{\mu,\mathbf{s},\mathbf{q},\mathbf{c},\sigma\}
$$
这样得到的带有注释的(~~什么意思？~~)点云可以使用可微分的基于图块的光栅化器$\cal R$和相机参数$\pi$高效的渲染成图像$\bf I$:
$$
I=\mathcal{R}(G,\pi)
$$
在下文中，我们将讨论如何使用2D CNN架构有效地生成3D高斯点云$G$。

* **Template-based 3D Gaussian Generation**

3D高斯是一种基于点的表示，本质上是**非结构化**的；这对生成任务提出了重大挑战，因为存在诸如顺序模糊或大面积空区域等问题。因此，我们遵循之前一些工作，将3D高斯装配到具有相应UV布局的模板网格上，











## 参考资料

> 
>

