---
title: Point-based-head-share

date: 2024-09-21 17:00:00 +0800

categories: [3D computer vision, Face, papers]

tags: [3D computer vision, Face, papers]

math: true

mermaid: true

description: Some papers about Point-based-head-share

---



## PointAvatar: Deformable Point-based Head Avatars from Videos

从随意的视频序列中创建逼真的可动画化和可灯光化头像的能力将在通信和娱乐领域开辟广泛的应用。目前的方法要么建立**在显式三维变形网格(3DMM)**上，要么利用神经隐式表征。前者受固定拓扑结构的限制，而后者易变形且渲染效率低。此外，现有的方法在颜色估计中纠缠光照，因此它们在新环境中重新渲染角色时受到限制。相比之下，我们提出了PointAvatar，这是一种可变形的基于点的表示，它将源颜色分解为内在反照率和法线相关的阴影。我们演示了PointAvatar在现有的网格表示和隐式表示之间架起了桥梁，将高质量的几何形状和外观与拓扑灵活性、易于变形和渲染效率相结合。我们表明，我们的方法能够使用来自多个来源的单目视频(包括手持智能手机，笔记本电脑网络摄像头和互联网视频)生成可动画的3D化身，在以前的方法失败的具有挑战性的情况下(例如，细发丝)实现最先进的质量，同时在训练中显着比竞争方法更有效。

## HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors

![image-20241022162456768](D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\image-20241022162456768.png)

在本文中，我们介绍了一种新颖的三维头像创建方法，这种方法能够从**很少的野外拍摄数据**中生成**高保真**和**可动画化**的三维头像。考虑到这一问题的**低约束性**，结合**先验知识**至关重要。因此，我们提出了一个框架，包括先验学习和头像创建两个阶段。先验学习阶段利用从**大规模多视角动态数据集**中获得的**三维头部先验知识**，而头像创建阶段则应用这些先验知识进行少量拍摄的个性化处理。我们的方法利用**基于3DGS的自动解码器网络**和**Part-based**的动态建模，有效地捕捉了这些先验。我们的方法利用身份共享编码和个人身份的个性化潜码来学习高斯点的属性。在头像创建阶段，我们利用反转和微调策略实现了快速的头部头像个性化。广泛的实验证明，我们的模型有效地利用了头部先验，并成功地将其推广到少镜头个性化场景中，实现了逼真的渲染质量、多视角一致性和稳定的动画效果。

### Contributions

* 我们介绍了一种新的框架，利用可推广的3D高斯先验，只使用几个输入图像，就可以快速3D头部头像个性化，。这些化身表现出高保真度和一致的动画质量。
* 我们的设计可以有效地利用part-based动态高斯头部先验，并将其推广为高品质的少样本头部个性化人头重建。

### Method

![image-20241022182526650](D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\Pipline.png)

我们的头像表示基于**自动解码器先验模型**，该模型可以从**多个身份**中学习**头部先验知识**，并用于从少数拍摄图像中创建头部头像。如上图所示，我们的表示建立在**基于点**的表示和Part-based的建模之上，其中**每个点只负责一个语义部分**。首先，我们初始化Part-based的特征点云，包括**1）Part-based的id代码**和2）基于跟踪网格的**特定高斯点的特征编码**。然后，我们进行动态建模，通过将特征点云输入Part-based的多层感知（MLP）中去回归得到用于渲染的高斯点的属性。最后，我们利用**卷积神经网络（CNN）模块来细化3DGS渲染**，以获得最终渲染图像。在下文中，我们将描述这些关键的组件。

#### Avatar Representation

* **Part-based Feature Point Cloud**

为了初始化基于tracked mesh$M$的Part-based Feature Point Cloud，我们首先利用**基于UV的初始化**[[FlashAvatar]]([USTC3DV/FlashAvatar-code: [CVPR 2024\] The official repo for FlashAvatar](https://github.com/USTC3DV/FlashAvatar-code))来获得n个初始的高斯点，同时每个像素都被绑在了mesh平面上的三角形中，与**Face-based**的初始化方式相比，这样的初始化有助于在头部区域上分布更均匀的基元。

然后，我们为点云设置初始特征。特征包含两种类型：

1. 特定点的特征编码$\mathbf{f}=\left\{\mathbf{f}_{i} \in \mathbb{R}^{c_{1}}\right\}_{i=1}^{n}$
2. 部分id编码$\mathbf{z}=\left\{\left\{\mathbf{z}_{j}^{l} \in \mathbb{R}^{c_{2}}\right\}_{l=1}^{p}\right\}_{j=1}^{k}$

其中$p$和$k$分别表示Part和id_number。点编码$\mathbf f$嵌入了id共享的先验并且id码$\mathbf z$用作自动解码器模型的**id_codebook**（~~这是啥~~）。**所有编码都是随机初始化的可学习参数**(参数化)。高斯点的部分由其父三角形确定，属于同一部分的高斯点共享所有的共享相同的id_code。

* **Part-based Dynamic Gaussian Attributes Modeling**

为了简单起见，我们使用$\mathbf f$和$\mathbf z$来表示属于特定部分$p$的逐点的特征。对于给定的$\mathbf f$和$\mathbf z$，通过这样的方式来进行动态局部的高斯属性:


$$
\mathcal{A}^g=f_p^{\mathcal{M}_1}(\mathbf{f},\mathbf{z}),\quad\mathbf{h}=f_p^{\mathcal{M}_2}(\mathbf{f},\mathbf{z},\mathbf{e},\mathcal{A}^g)
$$


其中$\mathcal{A}^g=\{\boldsymbol{\mu}^{\prime},\mathbf{r}^{\prime},\mathbf{S}^{\prime},\alpha\}$是除了位置之外的一个高斯点的局部属性，并且${\bf e}:=\,{\cal D}(\mu^{\prime})\,=\,{\cal T}(\mu^{\prime})\,-\mathcal{T}\left(\boldsymbol{\mu}_{\text {neutral }}^{\prime}\right)$是通过全局位姿点位置${\cal T}(\mu^{\prime})$减去全局中性点位置$$\mathcal{T}\left(\boldsymbol{\mu}_{\text {neutral }}^{\prime}\right)$$获得的点特定动态信号;$f_p^{\mathcal{M}_1}$和$f_p^{\mathcal{M}_2}$都是part-specific的多层神经网络。我们定义整体的动态建模过程为$\mathcal{A}^f=f_p^{\mathcal{M}}(\mathbf{f}, \mathbf{z})$，其中$\mathcal{A}^f=\mathcal{A}^g \cup\{\mathbf{h}\}$表示用于最终用于Splatting过程的高斯点的属性。

Part-based和动态的建模都有助于小样本情况下的表现，Part-based的建模允许特定的模块去学习特定部分的先验，从而更容易优化并获得更强大的先验知识。动态的建模采用**点特定的表达信号**${\bf e}$来**预测动态局部属性**，这比使用静态局部属性的[GaussianAvatars]([[2312.02069\] GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians](https://arxiv.org/abs/2312.02069))在捕捉动态细节方面更好。

* **Gaussian Splatting with CNN refinement**

受到[一些工作]([[2305.01190\] LatentAvatar: Learning Latent Expression Code for Expressive Neural Head Avatar](https://arxiv.org/abs/2305.01190))的启发，作者再次也采用了一个Scree-Space(~~为什么是这个名字~~)的CNN$f^{C}$来refine渲染的结果：


$$
\left[\mathbf{I}_{r g b}, \mathbf{I}_h\right]=\mathcal{R}\left(\mathcal{T}\left(\mathcal{A}_f, \mathcal{M}\right), \pi_{\mathbf{K}, \mathbf{E}}\right)
$$

$$
\mathbf{I}=f^{\mathcal{C}}\left(\left[\mathbf{I}_{r g b}, \mathbf{I}_h\right]\right)
$$



其中$\mathcal{A}^f=\left\{\mathcal{A}_i^f\right\}_{i=1}^n$代表最后所有点的高斯属性，$\bf I_{rgb}$是最终渲染出来的RGB图像，$\bf I_{h}$是用于CNN refinement的潜在特征图像。

与[之前工作]([YuelangX/Gaussian-Head-Avatar: [CVPR 2024\] Official repository for "Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians"](https://github.com/YuelangX/Gaussian-Head-Avatar))不同的是,我们不使用CNN来进行超分，而是保持输入和输出具有相同的分辨率，以进行细化；我们的目标是使用大规模的训练数据，使CNN能够捕捉到泛化的结构化外观先验知识，这对于3DGS-based表征方法是难以做到的，后面的实验分析也证明了，只用CNN来细化，得到的效果确实更加逼真，实现了few-shot的个性化操作。

* **Overall Representation**

至此，整个的头部化身被正式地定义为：


$$
\mathcal{H}:\left(\mathcal{M} ; f^{\mathcal{M}}, f^{\mathcal{C}}, \mathbf{f}, \mathbf{z}\right) \mapsto \mathbf{I}
$$



#### Head Prior Learning

我们高度依赖于头部的先验知识，以只有几个输入图像情况下无约束地实现高保真的化身重建。在各种先验知识中，我们的目标是从能获得的具有多个身份的多视图动态头部数据中学习高质量、可动画化和3D连续的头部先验知识。因此，先验学习阶段的目标是去使用GAPNet用id码$\mathbf{Z}_{1 \ldots k}$和其他优化的网络参数来学习对应的$k$个人头化身，在开始模型训练之前，我们首先对训练数据进行FLAME估计得到$\text{M}$。然后，我们使用这些数据来联合优化$\mathcal{M}, f^{\mathcal{M}},$$f^{\mathcal{C}},\mathrm{f},$和$\bf z$。这是总的损失函数：


$$
\mathcal{L}=\mathcal{L}_{rec}(\mathbf{I},\mathbf{I}^{*})+\mathcal{L}_{rec}(\mathbf{I}_{rgb},\mathbf{I}^{*})+\lambda_{m}\mathcal{L}_{rec}(\mathbf{I}_{m},\mathbf{I}_{m}^{*})+\mathcal{L}_{reg},
$$


其中$\mathcal{L}_{rec}$和$\mathcal{L}_{reg}$分别表示图像重建损失和训练正则化损失,真值图被表示为$\mathbf{I}^{*}$,为了提高嘴部区域的保真度，我们受[FlashAvatar]([USTC3DV/FlashAvatar-code: [CVPR 2024\] The official repo for FlashAvatar](https://github.com/USTC3DV/FlashAvatar-code))启发,进一步用的掩蔽的真实嘴部区域$\mathbf{I}_{m}^{*}$来监督掩蔽的渲染嘴部区域$\mathbf{I}_{m}$;具体来说，图像的重建损失为：


$$
\mathcal{L}_{rec}=\lambda_{l1}\mathcal{L}_{l1}+\lambda_{ssim}\mathcal{L}_{ssim}+\lambda_{lpips}\mathcal{L}_{lpips}
$$


与此同时，训练正则化损失为：


$$
\mathcal{L}_{reg}=\lambda_{\alpha}\mathcal{L}_{\alpha}+\lambda_{s}\mathcal{L}_{s}+\lambda_{\mu}\mathcal{L}_{\mu}+\lambda_{arap}\mathcal{L}_{arap},
$$


这其中包括了不透明度正则项$\mathcal{L}_{\alpha}=\|\mathbf{I}_{\alpha}\mathbf{-}\tilde{\mathbf{I}}_{mask}\|_{1}$,原始局部scale正则化$\mathcal{L}_s=\|\max(\mathbf{s},\epsilon_s)\|_2$,原始局部位置正则化$\mathcal{L}_\mu=\|\max(\boldsymbol{\mu},\epsilon_\mu)\|_2$,还有来自[变形领域经典论文]([igl.ethz.ch/projects/ARAP/arap_web.pdf](https://igl.ethz.ch/projects/ARAP/arap_web.pdf))(~~论文我也没看过~~)的ARAP损失$\mathcal{L}_{arap}$，....

#### Few-shot Personalization

在先验学习阶段之后，我们用GAPNet来编码动态头部先验知识;因此所有GAPNet学到的参数都可以作为强大的先验用于few-shot甚至one-shot的个性化人头重建。

在个性化重建之前，我们采用了一个**tracker**(~~这是啥东西，这个要学一下~~)获取输入图像的FLAME参数。给定具FLAME跟踪的输入图像，我们首先通过inversion从id_codebook中找到最相似的化身。具体地说，我们优化部件特定的线性组合权重$\textbf{w}\in\mathbb{R}^{\tilde{k}\times p\times1}$，以获得用于渲染与输入相似的化身的身份码$\mathbf{z}^{*} = \mathrm{softmax}(\mathbf{w})\odot\mathbf{z} \in \mathbb{R}^{k\times p\times c_{2}}$。在反演优化过程中，除了$\mathbf{w}$之外，我们保持网络的所有参数冻结。在形式上，给定目标身份的输入图像$\bf I ^{*}$，我们优化以呈现类似于目标身份的图像$\bf I$。

然后，我们开始微调以更新网络的参数，以便化身可以从输入中捕获目标身份的细节。我们通过**三种策略**在这个过程中利用先验知识。首先，我们对除**特征编码$\bf f$之外**的所有参数使用小的学习率。接下来，我们通过排除嘴部区域的微调过程来充分利用之前提取的Part-based的先验，因为用很少的输入来建模高度灵活的嘴部区域是十分有挑战性的。最后，我们应用视图正则化来防止目标视图的过拟合：具体地，我们将具有中性面$\{\mathbf{R}_i\}_{i=1}^m$的一些参考视图的微调结果约束为接近微调$\{\tilde{\mathbf{R}}_i\}_{i=1}^m$之前的渲染结果，其中$m$是生成的参考视图的数量。利用先验知识，我们的个性化化身实现了稳定的再现，同时保留了目标身份的细节。微调是通过最小化方程中的损失函数来进行的:


$$
\arg\min_\xi\mathcal{L}_f=\mathcal{L}(\mathbf{I},\mathbf{I}^*)+\lambda_{ref}\sum_{i=1}^m(\mathcal{L}(\mathbf{R}_i,\tilde{\mathbf{R}}_i)),
$$


其中$\xi$表示所有可学习的参数，$\lambda_{ref}$用于平衡不同的损失项。

# Generalizable and Animatable Gaussian Head Avatar

* **双提升方法**(Dual-lifting)

3D高斯的重建需要数百个高斯点，这对于从单图中重建是十分要命的，作者提出了一种**双提升**方法：首先通过冻结的DINOv2框架得到局部特征平面$F_{local}$,


$$
G_{pos}=[p_{s}+E_{Conv0}(F_{local})\cdot n_{s},\quad p_ {s}-E_{Conv1}(F_{local})\cdot n_{s}]
$$

$$
G_{c, o, s, r}=\left[E_{\text {Conv0 }}\left(F_{\text {local }}\right), \quad E_{\text {Conv1}}\left(F_{\text {local }}\right)\right]
$$



其中$p_{i}$是基于$I_{s}$的估计相机姿态映射的初始点平面，并且通过原点。$p_{i}$的大小为296 × 296，这与局部特征$F_{local}$一致。$E_{Conv0，1}$是卷积网络，$n_{s}$是$p_{s}$的法向向量，$G_{pos}$是3D高斯点的位置，$G_{c, o, s, r}$表示3D高斯的颜色、不透明度、比例和旋转。

# GGHead: Fast and Generalizable 3D Gaussian Heads

![image-20241023102829509](D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\gghead.png)

> 我们的方法可以基于3DGS生成不同的3D头部表示。被抽样的人展示了详细的几何形状和外观。生成和渲染都可以以全图像分辨率实时进行而不需要2D超分辨率网络。

#### Abstruct

从大型2D图像集合中学习3D头部先验信息是迈向高质量3D感知人体建模的重要一步。一个核心需求是一个可以很好地扩展到大规模数据集和大图像分辨率的高效的架构。不幸的是，现有的3D GAN由于其**相对较慢的训练和渲染速度**而难以扩展以**生成高分辨率**的样本，并且通常不得**不依赖于要牺牲全局3D一致性的2D超分辨率网络**。为了解决这些挑战，我们提出了GGHead，它在3D GAN框架内采用了最近的3dgs。为了生成3D表示，我们采用了一个强大的2D CNN生成器**来预测UV空间中模板头部mesh的的高斯点属性**。通过这种方式，GGHead利用了**模板UV布局的规律性**，大大促进了**预测非结构化3D高斯集**的挑战性任务。我们在**渲染的UV坐标上用一个新的总体变分损失**进一步提高了生成的3D表示的**几何保真度**。直观地说，这种正则化鼓励相邻的渲染像素应该源于模板的UV空间中的相邻高斯。总而言之，我们的pipeline可以通过在单视角的2D图片上生成训练的3D头部。我们提出的框架与在FFHQ数据集训练的现有的3D头部GAN的质量相匹配，同时速度更快且完全3D一致。因此，我们首次展示了以$1024^2$分辨率实时生成和渲染高质量的3D一致性头部。

#### Contributions

* 我们将3D Gaussian Heads参数化为UV maps(~~所以这个到底要怎么使用呢~~)，这种UV map可以使用高效的2D CNN结合一种新的UV总体变分正则化来生成，从而提高几何保真度。
* 我们的方法具有高度可扩展性，便于在1k分辨率下进行训练，同时实现实时样本生成和头部渲染

#### Method

![image-20241023105009718](D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\gghead_pipline.png)

> 我们在3D GAN公式中采用3DGS，实现了仅从2D图像中去学习3D头部的分布。为了构建3D高斯表示，我们利用了模板mesh的UV空间。然后CNN生成器**利用正态分布的隐编码**来预测每个高斯属性的2D映射。为了获得用于光栅化的实际3D高斯基元，我们对UV贴图进行均匀采样，并将基元相对于模板网格放置。然后，所生成的3D高斯表示被光栅化并由鉴别器监督。为了提高训练稳定性，特别是在对抗训练的早期阶段，我们通过$\mathcal{L}_{reg}^{pos},\mathcal{L}_{reg}^{scale}$和$\mathcal{L}_{reg}^{Opac}$来调整预测的$pos$、$scale$和$opacity$属性的偏移。此外，我们提出了**一种新的UV总体变分损失**$\mathcal{L}_{uv}$，通过强制UV渲染的平滑性以提高生成的3D头部的几何保真度。

* **3D Gaussian Splatting (3DGS)**

3DGS是一种基于点的场景表示，为每个点分配五个不同的属性:高斯中心$\mu\in\mathbb{R}^3$,大小$\mathbf{s}\in\mathbb{R}^3$,参数化为四元数的旋转表征$\mathbf{q}\in\mathbb{R}^4$,颜色$\mathbf{c}\in\mathbb{R}^3$和密度/不透明度$\sigma\in\mathbb{R}$:


$$
G^i=\{\mu,\mathbf{s},\mathbf{q},\mathbf{c},\sigma\}
$$


这样得到的带有注释的(~~什么意思？~~)点云可以使用可微分的基于图块的光栅化器$\cal R$和相机参数$\pi$高效的渲染成图像$\bf I$:


$$
I=\mathcal{R}(G,\pi)
$$


在下文中，我们将讨论如何使用2D CNN架构有效地生成3D高斯点云$G$。

* **Template-based 3D Gaussian Generation**

​	3D高斯是一种基于点的表示，本质上是**非结构化**的；这对生成任务提出了重大挑战，因为存在诸如顺序模糊或大面积空区域等问题。因此，我们遵循之前一些工作，将3D高斯装配到具有相应UV布局的模板网格上，这使得我们能够用一个有效的2D CNN骨干$\cal B$来预测高斯表征$\cal G$，这样大大简化了生成器的任务。形式上，我们为**每个高斯属性**生成**一个UV映射**$M$:


$$
M_\star=\cal B(z,\pi)
$$


其中$\star\in\{\text{POSITION, SCALE, ROTATION, COLOR, OPACITY}\}$，并且$\cal B$是一个StyleGAN2的生成器，用于将一个正态分布的隐变量$z\in\mathbb{R}^{512}$映射到UVmap$M\in\mathbb{R}^{256\times256\times14}$上。

​	对于位置的预测，我们添加了另一个可学习的权重初始为0的CNN层$\cal Z$：


$$
M_{position}\leftarrow\mathcal{Z}(M_{position})
$$


这确保了在训练开始时预测的位置偏移为0，从而导致在模板网格上预测高斯点（这算是提高训练稳定性的一个措施。

​	为了获得实际的3D高斯基元，我们均匀地对预测得到的2D图maps$M$进行采样(~~如何理解这里的采样呢~~)。每个高斯$\cal G^{i}$在模板的UV空间中有一个固定的坐标$x_{uv}^i\in[0,1]^2$，并且可以从$M$中查询其属性:


$$
G=\mathrm{GRIDSAMPLE}(M,x_{uv})
$$


其中$\mathrm{GRIDSAMPLE}()$运算符通过双线性插值在离散映射$M$中执行**查找**。𝑀抽样方案决定了将创建多少高斯模型，并在所有生成的人之间共享。请注意，生成器对采样了多少高斯分布基本上是不可知的，这意味着采样方案可以在训练期间改变，例如，以在以**较高分辨率渲染**时增加高斯数。

* **Gaussian Attribute Modeling**

为了确保预测得到的高斯属性可以保持在合理的范围内，我们采用激活函数。值得注意的是，对于预测的位置，我们在还加上了高斯点在mesh模板上的对应的3D位置$v^{i}$，并使用$\text{Tanh()}$激活函数来限制预测位置的偏移量：


$$
G_{position}^i\leftarrow v^i+\gamma_{pos}\text{Tanh}\left(G_{\textit{position}}^i\right)
$$


其中$\gamma_{pos}$是相对于模板网络的最大允许预测偏移；对于我们在FFHQ数据集上的实验，我们使用$\gamma_{pos} = 0.25$来表示高斯球最多可以从模板处远离25cm;这一步其实是至关重要的，因为它可以防止高斯球在对抗训练的早期阶段离开训练所用的视锥。

* **Gaussian Geometry Regularization**

简单地在对抗设置中预测3D高斯会导致较差的结果。其原因有两方面:

1. 在**鉴别器还未校准**的早期训练阶段，3D高斯函数对梯度更新的反应非常敏感。
2. 3D高斯的表征太过强大了。因此，生成器能够预测渲染时看起来似乎合理的场景，但底层几何体缺乏真实感。

我们发现，一组简单直觉的正则化可以将训练过程稳定化的同时也可以改善了基础几何。为了解决训练稳定性问题，我们对预测的位置偏移和高斯大小使用简单的L2正则化。为了提高生成的3D表征的几何保真度，我们建议在预测偏移上使用beta正则化，以及在UV的渲染上使应用一个新的总体变分损失。

**Gaussian Postion Regularization**：训练期间不稳定的一个常见原因是高斯运动太多，这里通过对预测的偏移施加正则化来阻止这种情况：


$$
\mathcal{L}_{reg}^{pos}=\|M_{position}\|_2
$$


这个损失项其实阻止了预测的高斯球去靠近模板mesh。

**Gaussian Scale Regularization**：与隐式场景表示（NeRF）相反，3D高斯模型对几个参数非常敏感。例如，一个高斯模型可能会在尺度上极大地增长，出现一个基元覆盖整个视锥的情况，这十分不利于训练的稳定性。我们在预测的高斯尺度上采用简单的正则化来避免这种退化的3D表示：


$$
\mathcal{L}_{reg}^{scale}=\|M_{scale}-\gamma_{scale}\|_{2}
$$


其中$\gamma_{scale}=e^{-5}$是高斯球刚好大到可以覆盖模板网格的目标大小。

**Gaussian Opacity Regularization：**为了提高生成的高斯球的几何保真度，我们对不透明度采用了beta正则化，鼓励高斯应该是完全透明或完全不透明的。

**UVTotalVariation Loss：**我们方法的一个简单实现可以产生具有很多个自由度的高斯表示。因此，在训练过程中，生成器可能会通过让颜色从更远的后面穿过来**“伪造”高频细节**，当然这不影响生成高质量的图像，但在渲染视频时产生明显的瑕疵。由于判别器是没有时间尺度的，因此上述的行为没有办法被”惩罚“以改善；因此，我们设计了一种新的正则化方案，以提高生成表征的**几何保真度**。我们正则化背后的直觉是，渲染图像中的相邻像素应该由在UV空间中也很接近的高斯球来建模，这自然会导致光滑的表面和惩罚孔。在渲染的UV图像的应用的总体变分正则化为：


$$
\hat{G}\leftarrow G   （1）
$$

$$
\hat{G}_{color}^i\leftarrow(u^i,v^i,0)   （2）
$$

$$
R_{uv}=\mathcal{R}(\hat{G},\pi)（3）
$$

$$
R_{uv}'=\frac{R_{uv}-(1-R_\alpha)}{R_\alpha}（4）
$$

$$
\mathcal{L}_{uv}=TV(R_{uv}^{\prime})（5）
$$



其中$u^{i},v^{i}=x_{uv}^{i}$是高斯点的UV坐标，$\pi$是相机位姿，以及$R_\alpha \in \mathbb{R}^{H\times W}$是包含来自光栅化的每像素累积。这用$R_{uv}$来表示UV Rendering;  上面的式（4）反转了在光栅化期间执行的$\alpha$合成过程，并且为了忽略正则化的透明度，阿尔法合成是必要的。否则，由于高斯的模糊性质，前景和背景之间的边界区域很可能包括半透明像素，这将一致地对正则化项做出贡献，激励前景高斯覆盖背景。通过利用这个损失，三维表征中的一些孔洞在训练过程中被缝合了起来，显著地改善了几何形状。



![image-20241023162324830](D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\uvlossof GGhead.png)



在没有$\mathcal{L}_{uv}$的情况下，预测高斯的几何通常是失败的，两种常见的失败案例是出现了在视频渲染中特别明显的高斯浮动线，以及通过让高斯从后脑勺照射而创建皮肤纹理的不正确表面。这两种情况都很容易在UV渲染的过程中检测到。而我们新的UV总体变分损失利用了这一事实，并通过强制让$R_{uv}$变得平滑来有效地解决这两个问题。

* **Training Pipeline**

我们在$256^2$的分辨率下生成UV贴图并且初始时候在每个uvmap纹理上采样一个高斯点，最终得到了$65k$个高斯点。我们开始在$256^2$渲染的分辨率下进行训练，并在7000k张训练图片后渐进地去增加分辨率。由于生成器与分辨率无关，因此在增加渲染分辨率时，我们**只需要为渲染器添加额外的层**。为了**渐进式增长**，我们在具有skip connection的判别器前面添加了一个CNN块，以确保在1000个卷积层图像的过程中平滑过渡并融合来自未训练层的贡献。在提高渲染分辨率时，我们还增加了高斯的数量，以便于建模更多的高频细节。为了实现这一点，我们将UV空间中高斯的采样密度从$256^2$增加到$512^2$，总共达到了$262k$个高斯球。高斯的突然增加导致预测暂时变得更加模糊。然而训练的稳定性不会受到不利影响，并且生成器可以快速适应更多高斯的存在。还要注意的是，对于任何给定的输入潜在变量，高斯函数永远不会同时都是活动的。生成器使用不透明度属性来根据需要启用和禁用高斯，从而避免模板网格所隐含的拓扑问题。实际上是平均约有一般的高斯球是活跃的。

最终生成器的优化项为：


$$
\mathcal{L}_{G}=\mathcal{L}_{adv}+\lambda_{p}\mathcal{L}_{reg}^{pos}+\lambda_{s}\mathcal{L}_{reg}^{scale}+\lambda_{o}\mathcal{L}_{reg}^{opac}+\lambda_{uv}\mathcal{L}_{uv}
$$




$$
\mathcal{L}_{adv}=\mathrm{SOFTPLUS}(-D(\mathcal{R}(\mathcal{G}),\pi))
$$



其中$\mathcal{L}_{adv}$是标准的非饱和GAN损失，这里是应用在了渲染3D的表征上,...,我们按照EG3D的方式训练模型，在两张48GB显存的A6000上训了12天(😵)。

#### Limitations and Futrue work

我们已经证明了GGHead可以以高质量生成和渲染3D头部。然而，所生成的3D表示仅给用户**提供了视点的控制**。对**面部表情**进行额外的控制将实现进一步的用例，例如对单个图像进行3D一致的表情编辑。由于我们的方法已经采用了粗模板网格，因此有理由以与GSM, Gaussian Avatars, Next3D等类似的方式对其进行扩展，例如，采用**FLAME和FFHQ**训练，并使用相应的表情代码。更进一步，我们方法的改进也许可以是在大型面部视频数据集上进行训练，最终实现仅从2D的数据就构建出逼真的3DMM来。

另一个改进的重心是通用性/泛化性。我们已经展示了如何使用我们的Pipline为人类头部的narrow domain获得3D生成模型。然而，将我们的方法扩展到其他领域，如ImageNet类别[Deng et al. 2009]可以学习更通用的高质量3D先验。这可以通过每个类别有一个模板网格并使网格本身可学习来完成。因此，该模型可以自己发现合适的模板。此外，可以采用类似Skorokhodov等人[2023]的方法来降低对特定于域的关键点检测器和对齐过程的要求。

>@article{kirschstein2024gghead,
>  title={GGHead: Fast and Generalizable 3D Gaussian Heads},
>  author={Kirschstein, Tobias and Giebenhain, Simon and Tang, Jiapeng and Georgopoulos, Markos and Nie{\ss}ner, Matthias},
>  journal={arXiv preprint arXiv:2406.09377},
>  year={2024}
>}

# 3D Gaussian Parametric Head Model

创建高保真3D人头头像对于VR/AR、临场感、数字人机界面和电影制作中的应用至关重要。最近的进展已经利用可变形的面部模型从容易访问的数据生成动画头部化身，在**低维参数空间**内表示不同的身份和表情。然而，现有的方法通常难以对**复杂的外观细节**进行建模，例如，**发型和配饰**，并且通常渲染质量低，效率也不高。本文介绍了一种**新的3D高斯参数化头部模型**，它采用3D高斯精确地表示人类头部的复杂性，允许精确控制身份和表情。此外，它还支持**丝滑的人脸肖像插值**以及**从单个图像重建详细的头部化身**。与以前的方法不同，高斯模型可以处理复杂的细节，实现不同外观和复杂表情的逼真表示。此外，本文提出了一个设计良好的训练框架，以确保顺利收敛，为学习丰富的内容提供了保证。我们的方法实现了高质量，照片般真实感渲染与实时效率，使其成为**参数化头部模型领域**的一个有价值的贡献。

![image-20241023200459428](D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\3dgphm.png)

> 我们利用了包括有多视图的视频数据和来自3D扫描的渲染图像数据的**混合数据集**来训练我们的模型。可以使用解耦的身份和表情代码来操纵训练的模型，以产生各种各样的高保真头部模型。当提供一张图像，我们的模型可以被调整以重建图像中的肖像，并根据任何其他所需的表情编辑表情。

#### Contribution

* 我们提出了**三维高斯参数化头部模型**，一种新的利用三维高斯作为表示的参数化头部模型，并实现了照片级的真实感渲染质量和实时渲染速度。
* 我们提出了一个设计的**训练策略**，以确保高斯模型稳定收敛，同时高效地**学习丰富的外观细节**和复杂的表情。
* 我们的3D高斯参数化头部模型可以根据**给定的单个图像**生成细节的高质量面部化身，并对其进行表情和身份编辑。



![image-20241023201509464](D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\pipline of 3dgphm.png)



> 我们的训练策略可以分为**初始化的引导几何模型**和最终的**3D高斯参数化头部模型**。每个模型的变形被进一步解耦为**与身份相关的**和**与表情相关的**变形。渲染涉及**使用DMTet将初始模型转换为网格和高斯模型的3D高斯溅射。**来自**两个模型的特征**最终通过卷积网络进行上采样上采样为高分辨率的肖像图像。在推理过程中，我们的输出仅来自高斯模型。

#### Method

与以往基于mesh或NeRF的模型相比，初始化和训练基于高斯的模型带来了独特的挑战,此外，我们还将提供训练细节，并演示如何在给定单个输入图像时使用我们的方法。

#### Data Preprocessing

使用三个数据集进行模型训练，包括多视图视频数据集NeRSamble和两个3D扫描数据集NPHM和FaceVerse。我们不直接扫描数据集的3D几何，而是把它们渲染成了多视图的图片，并且仅使用来自这三个数据集中的图片作为监督。为了更好地利用这些数据集，我们多了一些预处理工作：

1. 将图像的分辨率调整为$512^2$，并跟着调整相机参数。
2. 我们使用[BackgroundMattingV2]([PeterL1n/BackgroundMattingV2: Real-Time High-Resolution Background Matting](https://github.com/PeterL1n/BackgroundMattingV2))来提取NeRSamble数据集中的前景字符并记录掩码,而两个合成数据集不需要此步骤。
3. 我们使用[face alignment]([1adrianb/2D-and-3D-face-alignment: This repository implements a demo of the networks described in "How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)" paper.](https://github.com/1adrianb/2D-and-3D-face-alignment))来检测所有图像中的2D的landmarks。通过这些2Dlandmarks，我们为每个身份的每个表情都拟合一个BFM模型，并记录头部姿势和BFM的3D的landmarks

我们将使用上面处理的相机参数，图像，masks，BFM的头部姿势和BFM的3D地标来训练我们的模型。

#### Model Representation

高斯分布的表示由于其无序和非结构化的性质而带来了挑战，而且会导致在反向传播期间难以将梯度连续传播到空间中的相邻点。当**随机初始化高斯函数**时，这通常会导致**收敛失败**。另一方面，基于表面的表征方法，如mesh，只适合粗糙几何学习。**一个直接的想法是利用现有的3DMM**，如FLAME ，作为3DGS中点的初始位置。然而，该粗略初始化仍然未能将3D点的位置收敛到正确的位置，如下图所示。网络倾向于改变椭圆体的形状以实现合适的拟合结果，从而导致点云的几何形状不准确以及渲染图像中的模糊。



![image-20241023232708636](D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\flame_failure)



为了解决这个问题，需要更详细的初始化过程来使用3DGS来捕获不同的头部变化。 具体来说，我们借用[Gaussian Head Avatar]([Gaussian Head Avatar's Project Page](https://yuelangx.github.io/gaussianheadavatar/))的思路，并利用**隐式符号距离场（SDF）**表示来训练引导几何模型。 该引导几何模型作为高斯模型的初始值，为优化过程提供更有效的起点。 我们将初始模型定义为引导几何模型，然后将模型细化为 3D 高斯参数化头部模型。

* **Guiding Geometry Model**

Guiding Geometry Model通过接受身份编码$\cal z^{id}$和表情编码$\cal z^{exp}$作为输入，然后产生一个具有顶点 $V$,脸部$F$和每个顶点颜色$C$的网mesh，这个mesh与指定的id和表情对齐。为了实现这一目标，我们使用表示为$f_{me\boldsymbol{an}}(\cdot)$的 MLP 来隐式地建模 SDF，它表示平均几何形状：


$$
s,\gamma=f_{\boldsymbol{mean}}(x)
$$


其中$s$表示SDF值，$\gamma$表示最后一层的特征，$x$表示输入位置。 然后，我们通过[DMTet]([[2111.04276\] Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis](https://arxiv.org/abs/2111.04276))将隐式SDF转换为具有顶点位置 $V_{0}$、每顶点特征$\Gamma$和脸部$F$的显式的mesh。接下来，我们需要根据输入的身份编码$\cal z^{id}$将平均形状转换为中性表达形状。为了将身份信息注入mesh的顶点中，我们首先使用了一个投影MLP$f_{in\boldsymbol{j}}(\cdot)$，它将身份编码$\cal z^{id}$ 和每顶点特征$\Gamma$作为输入，并生成身份条件的每顶点特征向量$H=f_{\boldsymbol{inj}}(\boldsymbol{z^{id}},\Gamma)$. 随后，我们利用一个小型的MLP$f_{id}(\cdot)$去预测每个顶点的偏移$\delta V_{id}$，$\delta V_{id}$用于将平均形状转换为以身份编码$\cal z^{id}$为条件的中性表情形状。

​	在完成与身份(identity)相关的变形后，下一步是如何实现面部表情所引起的变形。 我们引入另一个小型的MLP $f_{exp}(\cdot)$。 这个MLP将上一步得到的特征向量$H$和表情编码$\cal z^{exp}$作为输入，然后输出是每个顶点的位移$\delta V_{exp}$。 使用这个位移，我们将顶点位置更新为$V_{can}$。 此外，我们将相同的特征向量$H$和表达式代码$\cal z^{exp}$输入到一个颜色MLP$f_{col}$中，以预测每个顶点的$32通道的$颜色$C$,这样的一个过程可以被描述为：


$$
V_{can}=V_0+\boldsymbol{f_{id}}(H)+\boldsymbol{f_{exp}}(H,\boldsymbol{z^{exp}}), C=\boldsymbol{f_{col}}(H,\boldsymbol{z^{exp}}).
$$


​	最后，我们利用数据预处理期间获得的估计头部姿态参数$R$和$T$将网格从规范空间转换到世界空间$V = R\cdot V_{can}+T$。在生成最终**mesh**的**顶点位置**，**颜色**和**脸部**$\{V,C,F\}$后,我们通过给定相机姿态的**可微光栅化**将网格渲染为 256 分辨率 32 通道的特征图$I_{F}$和掩模$M$。 随后，通过轻量级卷积上采样(超分)网络$\boldsymbol{\Psi}(\cdot)$将特征图解释为512分辨率的RGB$I_{hr}$图像。

* **3D Gaussian Parametric Head Model**

下面的高斯参数化模型也采用身份编码$\cal z^{id}$和表情编码$\cal z^{exp}$作为输入，生成3D高斯球的位置$X$、颜色$C$、比例$S$、旋转 $Q$和不透明度$A$。 与上面的Guiding Geometry Model类似，我们最初维护一个整体平均点云，其平均位置为$\bf X_{0}$。 然而，我们不再通过$ f_{mean}(x) $生成每顶点特征$\Gamma$。 相反，我们**直接生成$\Gamma$并将其绑定到高斯球上作为可优化变量** $\Gamma_{0}$。因为高斯球的数量在此阶段是固定的,所以这样做是可行的。 然后我们需要将平均点云转换为以身份编码$\cal z^{id}$为条件的中性表情点云。 为了实现这一点，我们使用了在引导几何模型中定义的相同的投影MLP$f_{in\boldsymbol{j}}(\cdot)$ 和身份变形场MLP $f_{id}(\cdot)$，它可以**生成编码了每个点的身份信息和预测每个点与身份相关偏移**的特征向量$\boldsymbol{f_{inj}}(\boldsymbol{z^{id}},\boldsymbol{\Gamma_{0}})$。然后，我们还需要预测表情编码$\cal z^{exp}$的conditioned位移。 与引导几何模型中提出的方法类似，得到的位置$X_{can}$和每个点的 32 通道颜色$C$ 可以描述为：


$$
X_{can}=\boldsymbol{X_{0}}+\boldsymbol{f_{id}}(H).+\boldsymbol{f_{exp}}(H,\boldsymbol{z^{exp}}),~C=\boldsymbol{f_{col}}(H,\boldsymbol{z^{exp}})
$$


与SDF和DMTet的表示不同，高斯有额外的属性需要预测。 在这里，我们引入了一种新的MLP来预测规范空间中的高斯属性，包括大小$S$、旋转$Q_{can}$、和不透明度$A$。为了确保生成结果的稳定性，**我们避免直接预测这些值**而是去预测它们相对于总体平均值$\{S_{\mathbf{0}},\boldsymbol{Q_{0}},\boldsymbol{A_{0}}\}$的偏移量$\{\delta S,\delta Q,\delta A\}$：


$$
\{S,Q_{can},A\}=\{\boldsymbol{S_{0}},\boldsymbol{Q_{0}},\boldsymbol{A_{0}}\}+\boldsymbol{f_{att}}(H,\boldsymbol{z^{exp}})
$$


接下来，我们利用数据预处理过程中获得的估计头部姿态参数$R$和$T$，将在规范化空间中的变量$X_{can}$和$Q_{can}$转换到世界空间:$X = R\cdot X_{can}+T, Q = R\cdot Q_{can}$。 对于模型渲染，我们利用可微渲染和神经渲染的技巧来生成图像。 生成的 3D 高斯参数${X, C, S, Q,A}$可以被身份编码$\cal z^{id}$和表情编码$\cal z^{exp}$为条件控制。最后，我们将该特征图输入到引导几何模型的相同上采样网络$\boldsymbol{\Psi}(\cdot)$中，以生成512分辨率的RGB图像。

​	在3D高斯参数化头部模型中，我们利用之前训练的Guiding Geometry Model来初始化我们的变量和网络，而不是随机启动它们或者从头开始训练。 具体来说，我们使用平均网格$V_{0}$的顶点位置来初始化高斯球的位置$X_{0}$。 同时如上所述，我们从一开始就从$f_{mean}(\cdot)$ 生成每顶点特征$\Gamma$并将其作为可优化变量$\Gamma _{0}$绑定到高斯点上。 此外，所有身份编码$z^{id}$、表达式代码$z^{exp}$和网络$\{\boldsymbol{f_{inj}}(\cdot),\boldsymbol{f_{id}}(\cdot),\boldsymbol{f_{exp}}(\cdot),\boldsymbol{f_{col}}(\cdot),\boldsymbol{\Psi}(\cdot)\}$都直接继承自引导几何模型。只有属性MLP$f_{\boldsymbol{att}}(\cdot)$ 是新引入的网络，因此是随机初始化的。 最后，按照原版3DGS初始化高斯属性的方式来初始化高斯属性$\{S_0,Q_0,A_0\}$的总体平均值

* **Loss Functions**

为了确保模型可以准确收敛，我们采用了各种损失函数作为约束，其中包括**基本光度损失**和**轮廓损失**来强制渲染的高分辨率图像$I_{hr}$和渲染的掩模$M$与真值的一致性：


$$
\mathcal{L}_{hr}=||I_{hr}-I_{gt}||_{1}, \mathcal{L}_{sil}=IOU(M,M_{gt}),
$$


其中$I_{gt}$代表真值RGB图像，$M_{gt}$代表地面实况掩模。我们通过引入L1损失进一步鼓励低分辨率特征图$I_{lr}$的前三个通道与真值RGB图像$I_{gt}$紧密匹配：


$$
\mathcal{L}_{lr}=||I_{lr}-I_{gt}||_1
$$


​	由表情引起的**几何变形**通常很复杂，而且不能仅通过图像监督来学习。 因此，我们使用 3D 地标为表达变形学习提供额外的粗略监督。 具体来说，我们在规范空间中定义3D地标$\bf P_{0}$，然后预测它们的位移并将其转换为世界空间并标记为$\bf P$,就像上面原始顶点$V_{0}$的变换一样。 然后，我们构建地标损失函数：


$$
\mathcal{L}_{lmk}=||\boldsymbol{P}-\boldsymbol P_{gt}||_2
$$


其中$\boldsymbol P_{gt}$表示地面实况3D地标,这些通过在预处理期间将BFM模型拟合到训练数据来估计。

此外，为了保证模型学习到的身份(id)和表情的解耦并最小化冗余，我们引入了以下正则化损失函数，旨在最小化两种变形的幅度：


$$
\mathcal{L}_{reg}=||\delta V_{id}||_{2}+||\delta V_{exp}||_{2}
$$


在Guiding Geometry Model的训练过程中，我们还构建了拉普拉斯平滑项$\mathcal{L}_{lap}$来惩罚表面噪声或破坏;总体而言，总损失函数可表示为：


$$
\mathcal{L}=\mathcal{L}_{hr}+\lambda_{sil}\mathcal{L}_{sil}+\lambda_{lr}\mathcal{L}_{lr}+\lambda_{lmk}\mathcal{L}_{lmk}+\lambda_{reg}\mathcal{L}_{reg}+\lambda_{lap}\mathcal{L}_{lap}
$$


其中,...,

在**3D高斯参数化头部模型**的训练阶段，我们还加入了感知损失，使得模型可以学习到更多高频细节$\mathcal{L}_{vgg} = VGG(I_{hr},I_{gt})$。 并且，与**训练引导几何模型**类似，我们强制特征图的前三个通道为 RGB 通道,并且引入landmarks损失函数和点位移正则化项损失(上面的$\cal L_{lr},L_{lmk},L_{reg}$)。因此，总体损失函数可以表示为：


$$
\mathcal{L}=\mathcal{L}_{hr}+\lambda_{vgg}\mathcal{L}_{vgg}+\lambda_{lr}\mathcal{L}_{lr}+\lambda_{lmk}\mathcal{L}_{lmk}+\lambda_{reg}\mathcal{L}_{reg}
$$


* **Inference Details**

**Image-based Fitting**：当输入单张RGB人像图像时，我们首先根据训练集的处理规则对图像进行对齐。 随后，我们采用梯度下降，使用光度损失$\cal L_{lr}$和$\cal L_{hr}$将3D高斯参数化头部模型渲染的图像拟合到该输入图像。此过程有助于回归身份编码$\cal z^{id}$和表情编码$\cal z^{exp}$。 我们针对两个潜在编码以$1\times10^{-3}$学习率仅进行200次优化的迭代。 接下来，我们固定身份编码$\cal z^{id}$和表情编码$\cal z^{exp}$，从而变量$H、X_{can}$也被固定。 我们使用相同的损失函数进一步优化颜色MLP$f_{col}(\cdot)$ 和代表当前特定对象几何形状的规范位置$X_{can}$。 在此步骤中，我们针对$f_{col}(\cdot)$ 和$X_{can}$以学习率$1\times10^{-4}$仅进行100次优化迭代,这个优化过程旨在添加一些训练模型本身无法恢复的细节，最终得到重建的头部模型。 整个过程总共迭代了300次，只需要30秒。

**Expression Editing: **给定提供要编辑表情的主体的Source图像和提供目标表情的目标肖像图像。 我们**首先通过优化获得源主体的头部模型**（如上述基于图像的拟合策略）。 然后对于目标人像图像，我们**也以同样的方式获取头部模型和对应的表情代码**。 最后，我们将目标表情代码$z^{exp}$输入到Source主题的头部模型中，这样就可以将Source图像的表情编辑为目标表情。























>```
>@inproceedings{xu2024gphm,
>title={3D Gaussian Parametric Head Model},
>author={Xu, Yuelang and Wang, Lizhen and Zheng, Zerong and Su, Zhaoqi and Liu, Yebin},
>booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
>year={2024}
>}
>```

## 参考资料

> 
>

