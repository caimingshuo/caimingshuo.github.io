---
title: Point-based-head-share

date: 2024-09-21 17:00:00 +0800

categories: [3D computer vision, Face]

tags: [3D computer vision, Face]

math: true

mermaid: true

description: 

---



#### PointAvatar: Deformable Point-based Head Avatars from Videos

The ability to create realistic animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting in the color estimation, thus they are limited in re-rendering the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos
from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-theart quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods.



从随意的视频序列中创建逼真的可动画化和可灯光化头像的能力将在通信和娱乐领域开辟广泛的应用。目前的方法要么建立**在显式三维变形网格(3DMM)**上，要么利用神经隐式表征。前者受固定拓扑结构的限制，而后者易变形且渲染效率低。此外，现有的方法在颜色估计中纠缠光照，因此它们在新环境中重新渲染角色时受到限制。相比之下，我们提出了PointAvatar，这是一种可变形的基于点的表示，它将源颜色分解为内在反照率和法线相关的阴影。我们演示了PointAvatar在现有的网格表示和隐式表示之间架起了桥梁，将高质量的几何形状和外观与拓扑灵活性、易于变形和渲染效率相结合。我们表明，我们的方法能够使用来自多个来源的单目视频(包括手持智能手机，笔记本电脑网络摄像头和互联网视频)生成可动画的3D化身，在以前的方法失败的具有挑战性的情况下(例如，细发丝)实现最先进的质量，同时在训练中显着比竞争方法更有效。

#### HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors

In this paper, we present a novel 3D head avatar creation approach capable of generalizing from few-shot in-the-wild data with high-fidelity and animatable robustness. Given the underconstrained nature of this problem, incorporating prior knowledge is essential. Therefore, we propose a
framework comprising prior learning and avatar creation phases. The prior learning phase leverages 3D head priors derived from a large-scale multi-view dynamic dataset, and the avatar creation phase applies these priors for fewshot personalization. Our approach effectively captures these priors by utilizing a Gaussian Splatting-based autodecoder network with part-based dynamic modeling. Our method employs identity-shared encoding with personalized latent codes for individual identities to learn the attributes of Gaussian primitives. During the avatar creation phase, we achieve fast head avatar personalization by leveraging inversion and fine-tuning strategies. Extensive experiments demonstrate that our model effectively exploits head priors and successfully generalizes them to few-shot personalization, achieving photo-realistic rendering quality, multi-view consistency, and stable animation.

在本文中，我们介绍了一种新颖的三维头像创建方法，这种方法能够从很少的野外拍摄数据中概括出高保真和可动画化的鲁棒性。考虑到这一问题的低约束性，结合先验知识至关重要。因此，我们提出了一个框架，包括先验学习和头像创建两个阶段。先验学习阶段利用从大规模多视角动态数据集中获得的三维头部先验知识，而头像创建阶段则应用这些先验知识进行少量拍摄的个性化处理。我们的方法利用基于高斯拼接的自动解码器网络和基于部件的动态建模，有效地捕捉了这些先验。我们的方法利用身份共享编码和个人身份的个性化潜码来学习高斯基元的属性。在头像创建阶段，我们利用反转和微调策略实现了快速的头部头像个性化。广泛的实验证明，我们的模型有效地利用了头部先验，并成功地将其推广到少镜头个性化，实现了逼真的渲染质量、多视角一致性和稳定的动画效果。

## 参考资料

> 
>

