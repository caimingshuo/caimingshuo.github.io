---
title: ECCV 2024 3D Gaussian Parametric Head Model

date: 2024-10-03 17:00:00 +0800

categories: [3D computer vision, Face, papers]

tags: [3D computer vision, Face, papers]

math: true

mermaid: true

description: Some papers about Point-based-head

---

## 3D Gaussian Parametric Head Model

创建高保真3D人头头像对于VR/AR、临场感、数字人机界面和电影制作中的应用至关重要。最近的进展已经利用可变形的面部模型从容易访问的数据生成动画头部化身，在**低维参数空间**内表示不同的身份和表情。然而，现有的方法通常难以对**复杂的外观细节**进行建模，例如，**发型和配饰**，并且通常渲染质量低，效率也不高。本文介绍了一种**新的3D高斯参数化头部模型**，它采用3D高斯精确地表示人类头部的复杂性，允许精确控制身份和表情。此外，它还支持**丝滑的人脸肖像插值**以及**从单个图像重建详细的头部化身**。与以前的方法不同，高斯模型可以处理复杂的细节，实现不同外观和复杂表情的逼真表示。此外，本文提出了一个设计良好的训练框架，以确保顺利收敛，为学习丰富的内容提供了保证。我们的方法实现了高质量，照片般真实感渲染与实时效率，使其成为**参数化头部模型领域**的一个有价值的贡献。

![image-20241023200459428](/imgs/3dv/3dv9/3dgphm.png)

> 我们利用了包括有多视图的视频数据和来自3D扫描的渲染图像数据的**混合数据集**来训练我们的模型。可以使用解耦的身份和表情代码来操纵训练的模型，以产生各种各样的高保真头部模型。当提供一张图像，我们的模型可以被调整以重建图像中的肖像，并根据任何其他所需的表情编辑表情。

#### Contribution

* 我们提出了**三维高斯参数化头部模型**，一种新的利用三维高斯作为表示的参数化头部模型，并实现了照片级的真实感渲染质量和实时渲染速度。
* 我们提出了一个设计的**训练策略**，以确保高斯模型稳定收敛，同时高效地**学习丰富的外观细节**和复杂的表情。
* 我们的3D高斯参数化头部模型可以根据**给定的单个图像**生成细节的高质量面部化身，并对其进行表情和身份编辑。

![image-20241023201509464](/imgs/3dv/3dv9/pipline of 3dgphm.png)

> 我们的训练策略可以分为**初始化的引导几何模型**和最终的**3D高斯参数化头部模型**。每个模型的变形被进一步解耦为**与身份相关的**和**与表情相关的**变形。渲染涉及**使用DMTet将初始模型转换为网格和高斯模型的3D高斯溅射。**来自**两个模型的特征**最终通过卷积网络进行上采样上采样为高分辨率的肖像图像。在推理过程中，我们的输出仅来自高斯模型。

#### Method

与以往基于mesh或NeRF的模型相比，初始化和训练基于高斯的模型带来了独特的挑战,此外，我们还将提供训练细节，并演示如何在给定单个输入图像时使用我们的方法。

#### Data Preprocessing

使用三个数据集进行模型训练，包括多视图视频数据集NeRSamble和两个3D扫描数据集NPHM和FaceVerse。我们不直接扫描数据集的3D几何，而是把它们渲染成了多视图的图片，并且仅使用来自这三个数据集中的图片作为监督。为了更好地利用这些数据集，我们多了一些预处理工作：

1. 将图像的分辨率调整为$512^2$，并跟着调整相机参数。
2. 我们使用[BackgroundMattingV2]([PeterL1n/BackgroundMattingV2: Real-Time High-Resolution Background Matting](https://github.com/PeterL1n/BackgroundMattingV2))来提取NeRSamble数据集中的前景字符并记录掩码,而两个合成数据集不需要此步骤。
3. 我们使用[face alignment](https://github.com/1adrianb/2D-and-3D-face-alignment)来检测所有图像中的2D的landmarks。通过这些2Dlandmarks，我们为每个身份的每个表情都拟合一个BFM模型，并记录头部姿势和BFM的3D的landmarks。

我们将使用上面处理的相机参数，图像，masks，BFM的头部姿势和BFM的3D地标来训练我们的模型。

#### Model Representation

高斯分布的表示由于其无序和非结构化的性质而带来了挑战，而且会导致在反向传播期间难以将梯度连续传播到空间中的相邻点。当**随机初始化高斯函数**时，这通常会导致**收敛失败**。另一方面，基于表面的表征方法，如mesh，只适合粗糙几何学习。**一个直接的想法是利用现有的3DMM**，如FLAME ，作为3DGS中点的初始位置。然而，该粗略初始化仍然未能将3D点的位置收敛到正确的位置，如下图所示。网络倾向于改变椭圆体的形状以实现合适的拟合结果，从而导致点云的几何形状不准确以及渲染图像中的模糊。

![image-20241023232708636](/imgs/3dv/3dv9/flame_failure.png)

为了解决这个问题，需要更详细的初始化过程来使用3DGS来捕获不同的头部变化。 具体来说，我们借用[Gaussian Head Avatar]([Gaussian Head Avatar's Project Page](https://yuelangx.github.io/gaussianheadavatar/))的思路，并利用**隐式符号距离场（SDF）**表示来训练引导几何模型。 该引导几何模型作为高斯模型的初始值，为优化过程提供更有效的起点。 我们将初始模型定义为引导几何模型，然后将模型细化为 3D 高斯参数化头部模型。

* **Guiding Geometry Model**

Guiding Geometry Model通过接受身份编码$\cal z^{id}$和表情编码$\cal z^{exp}$作为输入，然后产生一个具有顶点 $V$,脸部$F$和每个顶点颜色$C$的网mesh，这个mesh与指定的id和表情对齐。为了实现这一目标，我们使用表示为$f_{me\boldsymbol{an}}(\cdot)$的 MLP 来隐式地建模 SDF，它表示平均几何形状：


$$
s,\gamma=f_{\boldsymbol{mean}}(x)
$$


其中$s$表示SDF值，$\gamma$表示最后一层的特征，$x$表示输入位置。 然后，我们通过[DMTet]([[2111.04276\] Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis](https://arxiv.org/abs/2111.04276))将隐式SDF转换为具有顶点位置 $V_{0}$、每顶点特征$\Gamma$和脸部$F$的显式的mesh。接下来，我们需要根据输入的身份编码$\cal z^{id}$将平均形状转换为中性表达形状。为了将身份信息注入mesh的顶点中，我们首先使用了一个投影MLP$f_{in\boldsymbol{j}}(\cdot)$，它将身份编码$\cal z^{id}$ 和每顶点特征$\Gamma$作为输入，并生成身份条件的每顶点特征向量$H=f_{\boldsymbol{inj}}(\boldsymbol{z^{id}},\Gamma)$. 随后，我们利用一个小型的MLP$f_{id}(\cdot)$去预测每个顶点的偏移$\delta V_{id}$，$\delta V_{id}$用于将平均形状转换为以身份编码$\cal z^{id}$为条件的中性表情形状。

​	在完成与身份(identity)相关的变形后，下一步是如何实现面部表情所引起的变形。 我们引入另一个小型的MLP $f_{exp}(\cdot)$。 这个MLP将上一步得到的特征向量$H$和表情编码$\cal z^{exp}$作为输入，然后输出是每个顶点的位移$\delta V_{exp}$。 使用这个位移，我们将顶点位置更新为$V_{can}$。 此外，我们将相同的特征向量$H$和表达式代码$\cal z^{exp}$输入到一个颜色MLP$f_{col}$中，以预测每个顶点的$32通道的$颜色$C$,这样的一个过程可以被描述为：


$$
V_{can}=V_0+\boldsymbol{f_{id}}(H)+\boldsymbol{f_{exp}}(H,\boldsymbol{z^{exp}}), C=\boldsymbol{f_{col}}(H,\boldsymbol{z^{exp}}).
$$


​	最后，我们利用数据预处理期间获得的估计头部姿态参数$R$和$T$将网格从规范空间转换到世界空间$V = R\cdot V_{can}+T$。在生成最终**mesh**的**顶点位置**，**颜色**和**脸部**$\{V,C,F\}$后,我们通过给定相机姿态的**可微光栅化**将网格渲染为 256 分辨率 32 通道的特征图$I_{F}$和掩模$M$。 随后，通过轻量级卷积上采样(超分)网络$\boldsymbol{\Psi}(\cdot)$将特征图解释为512分辨率的RGB$I_{hr}$图像。

* **3D Gaussian Parametric Head Model**

下面的高斯参数化模型也采用身份编码$\cal z^{id}$和表情编码$\cal z^{exp}$作为输入，生成3D高斯球的位置$X$、颜色$C$、比例$S$、旋转 $Q$和不透明度$A$。 与上面的Guiding Geometry Model类似，我们最初维护一个整体平均点云，其平均位置为$\bf X_{0}$。 然而，我们不再通过$ f_{mean}(x) $生成每顶点特征$\Gamma$。 相反，我们**直接生成$\Gamma$并将其绑定到高斯球上作为可优化变量** $\Gamma_{0}$。因为高斯球的数量在此阶段是固定的,所以这样做是可行的。 然后我们需要将平均点云转换为以身份编码$\cal z^{id}$为条件的中性表情点云。 为了实现这一点，我们使用了在引导几何模型中定义的相同的投影MLP$f_{in\boldsymbol{j}}(\cdot)$ 和身份变形场MLP $f_{id}(\cdot)$，它可以**生成编码了每个点的身份信息和预测每个点与身份相关偏移**的特征向量$\boldsymbol{f_{inj}}(\boldsymbol{z^{id}},\boldsymbol{\Gamma_{0}})$。然后，我们还需要预测表情编码$\cal z^{exp}$的conditioned位移。 与引导几何模型中提出的方法类似，得到的位置$X_{can}$和每个点的 32 通道颜色$C$ 可以描述为：


$$
X_{can}=\boldsymbol{X_{0}}+\boldsymbol{f_{id}}(H).+\boldsymbol{f_{exp}}(H,\boldsymbol{z^{exp}}),~C=\boldsymbol{f_{col}}(H,\boldsymbol{z^{exp}})
$$


与SDF和DMTet的表示不同，高斯有额外的属性需要预测。 在这里，我们引入了一种新的MLP来预测规范空间中的高斯属性，包括大小$S$、旋转$Q_{can}$、和不透明度$A$。为了确保生成结果的稳定性，**我们避免直接预测这些值**而是去预测它们相对于总体平均值$\{S_{\mathbf{0}},\boldsymbol{Q_{0}},\boldsymbol{A_{0}}\}$的偏移量$\{\delta S,\delta Q,\delta A\}$：


$$
\{S,Q_{can},A\}=\{\boldsymbol{S_{0}},\boldsymbol{Q_{0}},\boldsymbol{A_{0}}\}+\boldsymbol{f_{att}}(H,\boldsymbol{z^{exp}})
$$


接下来，我们利用数据预处理过程中获得的估计头部姿态参数$R$和$T$，将在规范化空间中的变量$X_{can}$和$Q_{can}$转换到世界空间:$X = R\cdot X_{can}+T, Q = R\cdot Q_{can}$。 对于模型渲染，我们利用可微渲染和神经渲染的技巧来生成图像。 生成的 3D 高斯参数${X, C, S, Q,A}$可以被身份编码$\cal z^{id}$和表情编码$\cal z^{exp}$为条件控制。最后，我们将该特征图输入到引导几何模型的相同上采样网络$\boldsymbol{\Psi}(\cdot)$中，以生成512分辨率的RGB图像。

​	在3D高斯参数化头部模型中，我们利用之前训练的Guiding Geometry Model来初始化我们的变量和网络，而不是随机启动它们或者从头开始训练。 具体来说，我们使用平均网格$V_{0}$的顶点位置来初始化高斯球的位置$X_{0}$。 同时如上所述，我们从一开始就从$f_{mean}(\cdot)$ 生成每顶点特征$\Gamma$并将其作为可优化变量$\Gamma _{0}$绑定到高斯点上。 此外，所有身份编码$z^{id}$、表达式代码$z^{exp}$和网络$\{\boldsymbol{f_{inj}}(\cdot),\boldsymbol{f_{id}}(\cdot),\boldsymbol{f_{exp}}(\cdot),\boldsymbol{f_{col}}(\cdot),\boldsymbol{\Psi}(\cdot)\}$都直接继承自引导几何模型。只有属性MLP$f_{\boldsymbol{att}}(\cdot)$ 是新引入的网络，因此是随机初始化的。 最后，按照原版3DGS初始化高斯属性的方式来初始化高斯属性$\{S_0,Q_0,A_0\}$的总体平均值

* **Loss Functions**

为了确保模型可以准确收敛，我们采用了各种损失函数作为约束，其中包括**基本光度损失**和**轮廓损失**来强制渲染的高分辨率图像$I_{hr}$和渲染的掩模$M$与真值的一致性：


$$
\mathcal{L}_{hr}=||I_{hr}-I_{gt}||_{1}, \mathcal{L}_{sil}=IOU(M,M_{gt}),
$$


其中$I_{gt}$代表真值RGB图像，$M_{gt}$代表地面实况掩模。我们通过引入L1损失进一步鼓励低分辨率特征图$I_{lr}$的前三个通道与真值RGB图像$I_{gt}$紧密匹配：


$$
\mathcal{L}_{lr}=||I_{lr}-I_{gt}||_1
$$


​	由表情引起的**几何变形**通常很复杂，而且不能仅通过图像监督来学习。 因此，我们使用 3D 地标为表达变形学习提供额外的粗略监督。 具体来说，我们在规范空间中定义3D地标$\bf P_{0}$，然后预测它们的位移并将其转换为世界空间并标记为$\bf P$,就像上面原始顶点$V_{0}$的变换一样。 然后，我们构建地标损失函数：


$$
\mathcal{L}_{lmk}=||\boldsymbol{P}-\boldsymbol P_{gt}||_2
$$


其中$\boldsymbol P_{gt}$表示地面实况3D地标,这些通过在预处理期间将BFM模型拟合到训练数据来估计。

此外，为了保证模型学习到的身份(id)和表情的解耦并最小化冗余，我们引入了以下正则化损失函数，旨在最小化两种变形的幅度：


$$
\mathcal{L}_{reg}=||\delta V_{id}||_{2}+||\delta V_{exp}||_{2}
$$


在Guiding Geometry Model的训练过程中，我们还构建了拉普拉斯平滑项$\mathcal{L}_{lap}$来惩罚表面噪声或破坏;总体而言，总损失函数可表示为：


$$
\mathcal{L}=\mathcal{L}_{hr}+\lambda_{sil}\mathcal{L}_{sil}+\lambda_{lr}\mathcal{L}_{lr}+\lambda_{lmk}\mathcal{L}_{lmk}+\lambda_{reg}\mathcal{L}_{reg}+\lambda_{lap}\mathcal{L}_{lap}
$$


其中,...,

在**3D高斯参数化头部模型**的训练阶段，我们还加入了感知损失，使得模型可以学习到更多高频细节$\mathcal{L}_{vgg} = VGG(I_{hr},I_{gt})$。 并且，与**训练引导几何模型**类似，我们强制特征图的前三个通道为 RGB 通道,并且引入landmarks损失函数和点位移正则化项损失(上面的$\cal L_{lr},L_{lmk},L_{reg}$)。因此，总体损失函数可以表示为：


$$
\mathcal{L}=\mathcal{L}_{hr}+\lambda_{vgg}\mathcal{L}_{vgg}+\lambda_{lr}\mathcal{L}_{lr}+\lambda_{lmk}\mathcal{L}_{lmk}+\lambda_{reg}\mathcal{L}_{reg}
$$


* **Inference Details**

**Image-based Fitting**：当输入单张RGB人像图像时，我们首先根据训练集的处理规则对图像进行对齐。 随后，我们采用梯度下降，使用光度损失$\cal L_{lr}$和$\cal L_{hr}$将3D高斯参数化头部模型渲染的图像拟合到该输入图像。此过程有助于回归身份编码$\cal z^{id}$和表情编码$\cal z^{exp}$。 我们针对两个潜在编码以$1\times10^{-3}$学习率仅进行200次优化的迭代。 接下来，我们固定身份编码$\cal z^{id}$和表情编码$\cal z^{exp}$，从而变量$H、X_{can}$也被固定。 我们使用相同的损失函数进一步优化颜色MLP$f_{col}(\cdot)$ 和代表当前特定对象几何形状的规范位置$X_{can}$。 在此步骤中，我们针对$f_{col}(\cdot)$ 和$X_{can}$以学习率$1\times10^{-4}$仅进行100次优化迭代,这个优化过程旨在添加一些训练模型本身无法恢复的细节，最终得到重建的头部模型。 整个过程总共迭代了300次，只需要30秒。

**Expression Editing: **给定提供要编辑表情的主体的Source图像和提供目标表情的目标肖像图像。 我们**首先通过优化获得源主体的头部模型**（如上述基于图像的拟合策略）。 然后对于目标人像图像，我们**也以同样的方式获取头部模型和对应的表情代码**。 最后，我们将目标表情代码$z^{exp}$输入到Source主题的头部模型中，这样就可以将Source图像的表情编辑为目标表情。



>```
>@inproceedings{xu2024gphm,
>title={3D Gaussian Parametric Head Model},
>author={Xu, Yuelang and Wang, Lizhen and Zheng, Zerong and Su, Zhaoqi and Liu, Yebin},
>booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
>year={2024}
>}
>```

