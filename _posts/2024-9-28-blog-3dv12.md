---
title: GGHead: Fast and Generalizable 3D Gaussian Heads

date: 2024-09-28 17:00:00 +0800

categories: [3D computer vision, Face, papers]

tags: [3D computer vision, Face, papers]

math: true

mermaid: true

description: Some papers about Point-based-head

---

# GGHead: Fast and Generalizable 3D Gaussian Heads

![image-20241023102829509](/imgs/3dv/3dv9/gghead.png)

> 我们的方法可以基于3DGS生成不同的3D头部表示。被抽样的人展示了详细的几何形状和外观。生成和渲染都可以以全图像分辨率实时进行而不需要2D超分辨率网络。

## Abstruct

从大型2D图像集合中学习3D头部先验信息是迈向高质量3D感知人体建模的重要一步。一个核心需求是一个可以很好地扩展到大规模数据集和大图像分辨率的高效的架构。不幸的是，现有的3D GAN由于其**相对较慢的训练和渲染速度**而难以扩展以**生成高分辨率**的样本，并且通常不得**不依赖于要牺牲全局3D一致性的2D超分辨率网络**。为了解决这些挑战，我们提出了GGHead，它在3D GAN框架内采用了最近的3dgs。为了生成3D表示，我们采用了一个强大的2D CNN生成器**来预测UV空间中模板头部mesh的的高斯点属性**。通过这种方式，GGHead利用了**模板UV布局的规律性**，大大促进了**预测非结构化3D高斯集**的挑战性任务。我们在**渲染的UV坐标上用一个新的总体变分损失**进一步提高了生成的3D表示的**几何保真度**。直观地说，这种正则化鼓励相邻的渲染像素应该源于模板的UV空间中的相邻高斯。总而言之，我们的pipeline可以通过在单视角的2D图片上生成训练的3D头部。我们提出的框架与在FFHQ数据集训练的现有的3D头部GAN的质量相匹配，同时速度更快且完全3D一致。因此，我们首次展示了以$1024^2$分辨率实时生成和渲染高质量的3D一致性头部。

## Contributions

* 我们将3D Gaussian Heads参数化为UV maps(~~所以这个到底要怎么使用呢~~)，这种UV map可以使用高效的2D CNN结合一种新的UV总体变分正则化来生成，从而提高几何保真度。
* 我们的方法具有高度可扩展性，便于在1k分辨率下进行训练，同时实现实时样本生成和头部渲染

### Method

![image-20241023105009718](/imgs/3dv/3dv9/gghead_pipline.png)

我们在3D GAN公式中采用3DGS，实现了仅从2D图像中去学习3D头部的分布。为了构建3D高斯表示，我们利用了模板mesh的UV空间。然后CNN生成器**利用正态分布的隐编码**来预测每个高斯属性的2D映射。为了获得用于光栅化的实际3D高斯基元，我们对UV贴图进行均匀采样，并将基元相对于模板网格放置。然后，所生成的3D高斯表示被光栅化并由鉴别器监督。为了提高训练稳定性，特别是在对抗训练的早期阶段，我们通过$\mathcal{L}_{reg}^{pos},\mathcal{L}_{reg}^{scale}$和$\mathcal{L}_{reg}^{Opac}$来调整预测的$pos$、$scale$和$opacity$属性的偏移。此外，我们提出了**一种新的UV总体变分损失**$\mathcal{L}_{uv}$，通过强制UV渲染的平滑性以提高生成的3D头部的几何保真度。

* **3D Gaussian Splatting (3DGS)**

3DGS是一种基于点的场景表示，为每个点分配五个不同的属性:高斯中心$\mu\in\mathbb{R}^3$,大小$\mathbf{s}\in\mathbb{R}^3$,参数化为四元数的旋转表征$\mathbf{q}\in\mathbb{R}^4$,颜色$\mathbf{c}\in\mathbb{R}^3$和密度/不透明度$\sigma\in\mathbb{R}$:


$$
G^i=\{\mu,\mathbf{s},\mathbf{q},\mathbf{c},\sigma\}
$$


这样得到的带有注释的(~~什么意思？~~)点云可以使用可微分的基于图块的光栅化器$\cal R$和相机参数$\pi$高效的渲染成图像$\bf I$:


$$
I=\mathcal{R}(G,\pi)
$$


在下文中，我们将讨论如何使用2D CNN架构有效地生成3D高斯点云$G$。

* **Template-based 3D Gaussian Generation**

​	3D高斯是一种基于点的表示，本质上是**非结构化**的；这对生成任务提出了重大挑战，因为存在诸如顺序模糊或大面积空区域等问题。因此，我们遵循之前一些工作，将3D高斯装配到具有相应UV布局的模板网格上，这使得我们能够用一个有效的2D CNN骨干$\cal B$来预测高斯表征$\cal G$，这样大大简化了生成器的任务。形式上，我们为**每个高斯属性**生成**一个UV映射**$M$:


$$
M_\star=\cal B(z,\pi)
$$


其中$\star\in\{\text{POSITION, SCALE, ROTATION, COLOR, OPACITY}\}$，并且$\cal B$是一个StyleGAN2的生成器，用于将一个正态分布的隐变量$z\in\mathbb{R}^{512}$映射到UVmap$M\in\mathbb{R}^{256\times256\times14}$上。

​	对于位置的预测，我们添加了另一个可学习的权重初始为0的CNN层$\cal Z$：


$$
M_{position}\leftarrow\mathcal{Z}(M_{position})
$$


这确保了在训练开始时预测的位置偏移为0，从而导致在模板网格上预测高斯点（这算是提高训练稳定性的一个措施。

​	为了获得实际的3D高斯基元，我们均匀地对预测得到的2D图maps$M$进行采样(~~如何理解这里的采样呢~~)。每个高斯$\cal G^{i}$在模板的UV空间中有一个固定的坐标$x_{uv}^i\in[0,1]^2$，并且可以从$M$中查询其属性:


$$
G=\mathrm{GRIDSAMPLE}(M,x_{uv})
$$


其中$\mathrm{GRIDSAMPLE}()$运算符通过双线性插值在离散映射$M$中执行**查找**。𝑀抽样方案决定了将创建多少高斯模型，并在所有生成的人之间共享。请注意，生成器对采样了多少高斯分布基本上是不可知的，这意味着采样方案可以在训练期间改变，例如，以在以**较高分辨率渲染**时增加高斯数。

* **Gaussian Attribute Modeling**

为了确保预测得到的高斯属性可以保持在合理的范围内，我们采用激活函数。值得注意的是，对于预测的位置，我们在还加上了高斯点在mesh模板上的对应的3D位置$v^{i}$，并使用$\text{Tanh()}$激活函数来限制预测位置的偏移量：


$$
G_{position}^i\leftarrow v^i+\gamma_{pos}\text{Tanh}\left(G_{\textit{position}}^i\right)
$$


其中$\gamma_{pos}$是相对于模板网络的最大允许预测偏移；对于我们在FFHQ数据集上的实验，我们使用$\gamma_{pos} = 0.25$来表示高斯球最多可以从模板处远离25cm;这一步其实是至关重要的，因为它可以防止高斯球在对抗训练的早期阶段离开训练所用的视锥。

* **Gaussian Geometry Regularization**

简单地在对抗设置中预测3D高斯会导致较差的结果。其原因有两方面:

1. 在**鉴别器还未校准**的早期训练阶段，3D高斯函数对梯度更新的反应非常敏感。
2. 3D高斯的表征太过强大了。因此，生成器能够预测渲染时看起来似乎合理的场景，但底层几何体缺乏真实感。

我们发现，一组简单直觉的正则化可以将训练过程稳定化的同时也可以改善了基础几何。为了解决训练稳定性问题，我们对预测的位置偏移和高斯大小使用简单的L2正则化。为了提高生成的3D表征的几何保真度，我们建议在预测偏移上使用beta正则化，以及在UV的渲染上使应用一个新的总体变分损失。

**Gaussian Postion Regularization**：训练期间不稳定的一个常见原因是高斯运动太多，这里通过对预测的偏移施加正则化来阻止这种情况：


$$
\mathcal{L}_{reg}^{pos}=\|M_{position}\|_2
$$


这个损失项其实阻止了预测的高斯球去靠近模板mesh。

**Gaussian Scale Regularization**：与隐式场景表示（NeRF）相反，3D高斯模型对几个参数非常敏感。例如，一个高斯模型可能会在尺度上极大地增长，出现一个基元覆盖整个视锥的情况，这十分不利于训练的稳定性。我们在预测的高斯尺度上采用简单的正则化来避免这种退化的3D表示：


$$
\mathcal{L}_{reg}^{scale}=\|M_{scale}-\gamma_{scale}\|_{2}
$$


其中$\gamma_{scale}=e^{-5}$是高斯球刚好大到可以覆盖模板网格的目标大小。

**Gaussian Opacity Regularization：**为了提高生成的高斯球的几何保真度，我们对不透明度采用了beta正则化，鼓励高斯应该是完全透明或完全不透明的。

**UVTotalVariation Loss：**我们方法的一个简单实现可以产生具有很多个自由度的高斯表示。因此，在训练过程中，生成器可能会通过让颜色从更远的后面穿过来**“伪造”高频细节**，当然这不影响生成高质量的图像，但在渲染视频时产生明显的瑕疵。由于判别器是没有时间尺度的，因此上述的行为没有办法被”惩罚“以改善；因此，我们设计了一种新的正则化方案，以提高生成表征的**几何保真度**。我们正则化背后的直觉是，渲染图像中的相邻像素应该由在UV空间中也很接近的高斯球来建模，这自然会导致光滑的表面和惩罚孔。在渲染的UV图像的应用的总体变分正则化为：


$$
\hat{G}\leftarrow G   （1）
$$

$$
\hat{G}_{color}^i\leftarrow(u^i,v^i,0)   （2）
$$

$$
R_{uv}=\mathcal{R}(\hat{G},\pi)（3）
$$

$$
R_{uv}'=\frac{R_{uv}-(1-R_\alpha)}{R_\alpha}（4）
$$

$$
\mathcal{L}_{uv}=TV(R_{uv}^{\prime})（5）
$$



其中$u^{i},v^{i}=x_{uv}^{i}$是高斯点的UV坐标，$\pi$是相机位姿，以及$R_\alpha \in \mathbb{R}^{H\times W}$是包含来自光栅化的每像素累积。这用$R_{uv}$来表示UV Rendering;  上面的式（4）反转了在光栅化期间执行的$\alpha$合成过程，并且为了忽略正则化的透明度，阿尔法合成是必要的。否则，由于高斯的模糊性质，前景和背景之间的边界区域很可能包括半透明像素，这将一致地对正则化项做出贡献，激励前景高斯覆盖背景。通过利用这个损失，三维表征中的一些孔洞在训练过程中被缝合了起来，显著地改善了几何形状。

![image-20241023162324830](/imgs/3dv/3dv9/uvlossof GGhead.png)

在没有$\cal L_{uv}$的情况下，预测高斯的几何通常是失败的，两种常见的失败案例是出现了在视频渲染中特别明显的高斯浮动线，以及通过让高斯从后脑勺照射而创建皮肤纹理的不正确表面。这两种情况都很容易在UV渲染的过程中检测到。而我们新的UV总体变分损失利用了这一事实，并通过强制让$R_{uv}$变得平滑来有效地解决这两个问题。

* **Training Pipeline**

我们在$256^2$的分辨率下生成UV贴图并且初始时候在每个uvmap纹理上采样一个高斯点，最终得到了$65k$个高斯点。我们开始在$256^2$渲染的分辨率下进行训练，并在7000k张训练图片后渐进地去增加分辨率。由于生成器与分辨率无关，因此在增加渲染分辨率时，我们**只需要为渲染器添加额外的层**。为了**渐进式增长**，我们在具有skip connection的判别器前面添加了一个CNN块，以确保在1000个卷积层图像的过程中平滑过渡并融合来自未训练层的贡献。在提高渲染分辨率时，我们还增加了高斯的数量，以便于建模更多的高频细节。为了实现这一点，我们将UV空间中高斯的采样密度从$256^2$增加到$512^2$，总共达到了$262k$个高斯球。高斯的突然增加导致预测暂时变得更加模糊。然而训练的稳定性不会受到不利影响，并且生成器可以快速适应更多高斯的存在。还要注意的是，对于任何给定的输入潜在变量，高斯函数永远不会同时都是活动的。生成器使用不透明度属性来根据需要启用和禁用高斯，从而避免模板网格所隐含的拓扑问题。实际上是平均约有一般的高斯球是活跃的。

最终生成器的优化项为：


$$
\mathcal{L}_{G}=\mathcal{L}_{adv}+\lambda_{p}\mathcal{L}_{reg}^{pos}+\lambda_{s}\mathcal{L}_{reg}^{scale}+\lambda_{o}\mathcal{L}_{reg}^{opac}+\lambda_{uv}\mathcal{L}_{uv}
$$




$$
\mathcal{L}_{adv}=\mathrm{SOFTPLUS}(-D(\mathcal{R}(\mathcal{G}),\pi))
$$



其中$\mathcal{L}_{adv}$是标准的非饱和GAN损失，这里是应用在了渲染3D的表征上,...,我们按照EG3D的方式训练模型，在两张48GB显存的A6000上训了12天(😵)。

### Limitations and Futrue work

我们已经证明了GGHead可以以高质量生成和渲染3D头部。然而，所生成的3D表示仅给用户**提供了视点的控制**。对**面部表情**进行额外的控制将实现进一步的用例，例如对单个图像进行3D一致的表情编辑。由于我们的方法已经采用了粗模板网格，因此有理由以与GSM, Gaussian Avatars, Next3D等类似的方式对其进行扩展，例如，采用**FLAME和FFHQ**训练，并使用相应的表情代码。更进一步，我们方法的改进也许可以是在大型面部视频数据集上进行训练，最终实现仅从2D的数据就构建出逼真的3DMM来。

另一个改进的重心是通用性/泛化性。我们已经展示了如何使用我们的Pipline为人类头部的narrow domain获得3D生成模型。然而，将我们的方法扩展到其他领域，如ImageNet类别[Deng et al. 2009]可以学习更通用的高质量3D先验。这可以通过每个类别有一个模板网格并使网格本身可学习来完成。因此，该模型可以自己发现合适的模板。此外，可以采用类似Skorokhodov等人[2023]的方法来降低对特定于域的关键点检测器和对齐过程的要求。

>@article{kirschstein2024gghead,
>  title={GGHead: Fast and Generalizable 3D Gaussian Heads},
>  author={Kirschstein, Tobias and Giebenhain, Simon and Tang, Jiapeng and Georgopoulos, Markos and Nie{\ss}ner, Matthias},
>  journal={arXiv preprint arXiv:2406.09377},
>  year={2024}
>}

