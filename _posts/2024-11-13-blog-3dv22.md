---
title: SIGGRAPH 2024 URAvatar

date: 2024-10-30 17:00:00 +0800

categories: [3D computer vision, Face, papers]

tags: [3D computer vision, Face, papers]

math: true

mermaid: true

description: URAvatar
---

# URAvatar: Universal Relightable Gaussian Codec Avatars

![image-20241113220943678](D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\URAvatar.png)

>**URAvatar**  我们的方法可以通过手机一次的扫描（左）就创建可驱动和可重光照的逼真头部头像。 重建的化身可以在不同的照明下实时一致地跨身份驱动（右）。  https://junxuan-li.github.io/urgca-website/

### Abstract

我们提出了一种新方法，可以通过未知照明的手机拍照来创建逼真且可重新照明的头部头像。 重建的头像可以在不同环境的全局照明下实时驱动和重新点亮。 与通过逆向渲染估计反射率等参数的现有方法不同，我们的方法直接对可学习的辐射传输进行建模，该传输以有效的方式合并全局光传输以进行实时渲染。 然而，学习如此复杂的可以跨身份,可泛化的光传输并非易事。在单一环境中只用手机扫描缺乏足够的信息来推理出在一般环境中头部的外观。 为了解决这个问题，我们构建了一个以3D高斯为代表的通用可重光照的化身模型。我们使用可控点光源对数百个高质量多视图人体扫描进行训练。使用高分辨率的几何引导可以进一步提高重建精度和泛化能力。 训练完成后，我们使用逆渲染对手机扫描上的预训练模型进行微调，以获得个性化的可重光照的人头。 我们的实验证明了我们设计的有效性，优于现有方法，同时保留了**实时渲染**功能。

### contributions

* 我们引入了一个通用的可重新照明的化身先验模型，该模型是通过多视图和多光系统从数百个动态性能捕获中学习的。
* 我们通过手机扫描构建了一个可驱动的头部头像，可以通过全局光传输实时渲染和重新点亮。
* 捕获系统和评估协议被用于测量连续照明下重新照明的准确性。

### METHOD

![image-20241114103557508](C:\Users\Mingshuo Cai\AppData\Roaming\Typora\typora-user-images\image-20241114103557508.png)

>a. 我们采用了大量可重光照的多视图面部表演语料库来训练可以生成**volumetric avatar representations**的跨身份解码器$\cal D$。
>
>b. 给定一个未知身份的单个手机扫描图片，我们可以重建头部姿势、几何形状和反射纹理，并微调了我们预先训练的可重新点亮的先验模型。
>
>c. 我们对最终模型提供了对重照明、gaze和颈部控制的分离控制。

### METHOD

基于我们之前在Relightable Gaussian Codec Avatars中提出的几何和外观表示，我们首先描述了 3D 高斯和可学习辐射传输的基础。 然后我们讨论如何扩展Gaussian Codec Avatars以使用多身份训练数据构建通用的可重新点亮先验。 最后，我们提供了微调方法的详细信息，以使用通用可重光照的先验通过手机扫描去创建个性化可重光照的模型

#### Preliminaries: Relightable 3D Gaussians

头像被表示为3D高斯的集合$\mathbf{g}_k=\{\mathbf{t}_k,\mathbf{q}_k,\mathbf{s}_k,o_k,\mathbf{c}_k\}$，参数包括平移向量$\mathbf{t}_k\in\mathbb{R}^3$，由单位四元数参数化的旋转$\mathbf{q}_k\in\mathbb{R}^4$，沿着三个正交轴的大小因子$\mathbf{s}_k\in\mathbb{R}_+^3$，还有一个密度值$o_k\in\mathbb{R}_+$和颜色$\mathbf{c}_k\in\mathbb{R}_+^3$,根据3DGS的原理，这些高斯球可以实时地以很高的分辨率渲染。

我们的外观模型基于[learnable radiance transfer]()，为了对不同光照下的外观进行建模，预先计算地辐射传输 (PRT) 将渲染方程的积分分解为外部照明$L(\boldsymbol{\omega})$和内在辐射传输$T(\mathbf{p},\boldsymbol{\omega},\boldsymbol{\omega}^o)$的乘积。Saito等人进一步扩展了PRT，通过直接从多视图和多光源捕获数据中学习传递函数的参数，并将$T(\mathbf{p},\boldsymbol{\omega},\boldsymbol{\omega}^o)$分解为漫反射项（与观察方向无关）和镜面反射项
$$
\begin{aligned}
\mathbf{c}(\mathbf{p},\boldsymbol{\omega}^o)& =\int_{\mathbb{S}^2}L(\boldsymbol{\omega})\cdot T(\mathbf{p},\boldsymbol{\omega},\boldsymbol{\omega}^o)d\boldsymbol{\omega}, \\
&=\int_{\mathbb{S}^2}L(\boldsymbol{\omega})\cdot\left(T^\text{diffuse}(\mathbf{p},\boldsymbol{\omega})+T^\text{specular}(\mathbf{p},\boldsymbol{\omega},\boldsymbol{\omega}^o)\right)d\boldsymbol{\omega},
\end{aligned}
$$
其中$c$是沿$\omega^{o}$方向，在位置$\cal p$处的出射辐射率，$L$是入射光。特别的，每个高斯$\mathbf{c}_{k}$的出射辐射率被分解为与视图无关的漫反射项和与视图相关的镜面反射项$\mathbf{c}_k=\mathbf{c}_k^\text{diffuse}+\mathbf{c}_k^\text{specular}$。

漫反射颜色是通过入射辐射率和固有辐射率传输的积分来计算的，两者均由球谐函数 (SH) 参数化：
$$
\mathbf{c}_k^\mathrm{diffuse}=\boldsymbol{\rho}_k\odot\sum_{i=1}^{(n+1)^2}\mathbf{L}_i\odot\mathbf{d}_k^i,
$$
其中$\mathbf{L}_{i}$表示入射光的$n$阶球谐函数 (SH) 系数中的第$i$个元素，$d_{k}^{i}$表示可学习辐射传递函数的$n$阶球谐系数中的第$i$个元素，$\boldsymbol{\rho}_{k}$是基础反照率颜色。 这些项针对RGB通道单独建模, 镜面反射表示为球面高斯$G_s(\boldsymbol{\omega};\mathbf{a},\sigma)$，其中波瓣中心方向为$\bf a$，粗糙度为$\sigma$：
$$
\mathbf{c}_k^\mathrm{specular}(\boldsymbol{\omega}_k^o)=v_k(\boldsymbol{\omega}_k^o)\int_{\mathbb{S}^2}\mathbf{L}(\boldsymbol{\omega})G_s(\boldsymbol{\omega};\mathbf{a}_k,\sigma_k)\mathrm{d}\boldsymbol{\omega},\\\mathbf{a}_k=2(\boldsymbol{\omega}_k^o\cdot\mathbf{n}_k)\mathbf{n}_k-\boldsymbol{\omega}_k^o.
$$
这里，$v_k(\boldsymbol{\omega}_k^o)\in(0,1)$是一个可学习的依赖于视图的可见性项，它解释了菲涅尔反射、遮挡和几何衰减，$\omega_k^o\in\mathbb{R}^3$是在高斯中心评估的观察方向，$\bf n_{k}$是每个高斯的依赖于视图的法线。

#### Universal Relightable Prior Model

受到之前工作的启发，我们采用身份条件超网络生成特定于一个人的头像。 特别是，超参数网络将身份特征作为输入，并为每个主体的头像解码器生成特定于人的网络权重的子集。 该解码器生成与输入头部状态（面部表情、注视方向和颈部旋转）以及输入照明环境和视点相对应的可重新照明的 3D 高斯。 我们在图 2（a）中展示了概述。

<img src="D:\Github\caimingshuo.github.io\imgs\3dv\3dv9\URAvatar_pip2.png" alt="image-20241114154901570" style="zoom:80%;" />



