---
title: CVPR 2024 HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors

date: 2024-09-23 17:00:00 +0800

categories: [3D computer vision, Face, papers]

tags: [3D computer vision, Face, papers]

math: true

mermaid: true

description: paper

---

## HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors

![image-20241022162456768](/imgs/3dv/3dv9/image-20241022162456768.png)

在本文中，我们介绍了一种新颖的三维头像创建方法，这种方法能够从**很少的野外拍摄数据**中生成**高保真**和**可动画化**的三维头像。考虑到这一问题的**低约束性**，结合**先验知识**至关重要。因此，我们提出了一个框架，包括先验学习和头像创建两个阶段。先验学习阶段利用从**大规模多视角动态数据集**中获得的**三维头部先验知识**，而头像创建阶段则应用这些先验知识进行少量拍摄的个性化处理。我们的方法利用**基于3DGS的自动解码器网络**和**Part-based**的动态建模，有效地捕捉了这些先验。我们的方法利用身份共享编码和个人身份的个性化潜码来学习高斯点的属性。在头像创建阶段，我们利用反转和微调策略实现了快速的头部头像个性化。广泛的实验证明，我们的模型有效地利用了头部先验，并成功地将其推广到少镜头个性化场景中，实现了逼真的渲染质量、多视角一致性和稳定的动画效果。

## Contributions

* 我们介绍了一种新的框架，利用可推广的3D高斯先验，只使用几个输入图像，就可以快速3D头部头像个性化，。这些化身表现出高保真度和一致的动画质量。
* 我们的设计可以有效地利用part-based动态高斯头部先验，并将其推广为高品质的少样本头部个性化人头重建。

## Method

![image-20241022182526650](/imgs/3dv/3dv9/Pipline.png)

我们的头像表示基于**自动解码器先验模型**，该模型可以从**多个身份**中学习**头部先验知识**，并用于从少数拍摄图像中创建头部头像。如上图所示，我们的表示建立在**基于点**的表示和Part-based的建模之上，其中**每个点只负责一个语义部分**。首先，我们初始化Part-based的特征点云，包括**1）Part-based的id代码**和2）基于跟踪网格的**特定高斯点的特征编码**。然后，我们进行动态建模，通过将特征点云输入Part-based的多层感知（MLP）中去回归得到用于渲染的高斯点的属性。最后，我们利用**卷积神经网络（CNN）模块来细化3DGS渲染**，以获得最终渲染图像。在下文中，我们将描述这些关键的组件。

### Avatar Representation

* **Part-based Feature Point Cloud**

为了初始化基于tracked mesh$M$的Part-based Feature Point Cloud，我们首先利用**基于UV的初始化**[[FlashAvatar]]([USTC3DV/FlashAvatar-code: [CVPR 2024\] The official repo for FlashAvatar](https://github.com/USTC3DV/FlashAvatar-code))来获得n个初始的高斯点，同时每个像素都被绑在了mesh平面上的三角形中，与**Face-based**的初始化方式相比，这样的初始化有助于在头部区域上分布更均匀的基元。

然后，我们为点云设置初始特征。特征包含两种类型：

1. 特定点的特征编码   $\mathbf{f}=\left\{\mathbf{f}_{i} \in \mathbb{R}^{c_{1}}\right\}_{i=1}^{n}$
2. 部分id编码       $\mathbf{z}=\left\{\left\{\mathbf{z}_{j}^{l} \in \mathbb{R}^{c_{2}}\right\}_{l=1}^{p}\right\}_{j=1}^{k}$

其中$p$和$k$分别表示Part和id_number。点编码$\mathbf f$嵌入了id共享的先验并且id码$\mathbf z$用作自动解码器模型的**id_codebook**（~~这是啥~~）。**所有编码都是随机初始化的可学习参数**(参数化)。高斯点的部分由其父三角形确定，属于同一部分的高斯点共享所有的共享相同的id_code。

* **Part-based Dynamic Gaussian Attributes Modeling**

为了简单起见，我们使用$\mathbf f$和$\mathbf z$来表示属于特定部分$p$的逐点的特征。对于给定的$\mathbf f$和$\mathbf z$，通过这样的方式来进行动态局部的高斯属性:



$$
\mathcal{A}^g=f_p^{\mathcal{M}_1}(\mathbf{f},\mathbf{z}),\quad\mathbf{h}=f_p^{\mathcal{M}_2}(\mathbf{f},\mathbf{z},\mathbf{e},\mathcal{A}^g)
$$



其中$\mathcal{A}^g=\{\boldsymbol{\mu}^{\prime},\mathbf{r}^{\prime},\mathbf{S}^{\prime},\alpha\}$是除了位置之外的一个高斯点的局部属性，并且${\bf e}:=\,{\cal D}(\mu^{\prime})\,=\,{\cal T}(\mu^{\prime})\,-\mathcal{T}\left(\boldsymbol{\mu}_{\text {neutral }}^{\prime}\right)$是通过全局位姿点位置${\cal T}(\mu^{\prime})$减去全局中性点位置$$\mathcal{T}\left(\boldsymbol{\mu}_{\text {neutral }}^{\prime}\right)$$获得的点特定动态信号;$f_p^{\mathcal{M}_1}$和$f_p^{\mathcal{M}_2}$都是part-specific的多层神经网络。我们定义整体的动态建模过程为$\mathcal{A}^f=f_p^{\mathcal{M}}(\mathbf{f}, \mathbf{z})$，其中$\mathcal{A}^f=\mathcal{A}^g \cup\{\mathbf{h}\}$表示用于最终用于Splatting过程的高斯点的属性。

Part-based和动态的建模都有助于小样本情况下的表现，Part-based的建模允许特定的模块去学习特定部分的先验，从而更容易优化并获得更强大的先验知识。动态的建模采用**点特定的表达信号**${\bf e}$来**预测动态局部属性**，这比使用静态局部属性的[GaussianAvatars]([[2312.02069\] GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians](https://arxiv.org/abs/2312.02069))在捕捉动态细节方面更好。

* **Gaussian Splatting with CNN refinement**

受到[一些工作]([[2305.01190\] LatentAvatar: Learning Latent Expression Code for Expressive Neural Head Avatar](https://arxiv.org/abs/2305.01190))的启发，作者再次也采用了一个Scree-Space(~~为什么是这个名字~~)的CNN$f^{C}$来refine渲染的结果：



$$
\left[\mathbf{I}_{r g b}, \mathbf{I}_h\right]=\mathcal{R}\left(\mathcal{T}\left(\mathcal{A}_f, \mathcal{M}\right), \pi_{\mathbf{K}, \mathbf{E}}\right)
$$

$$
\mathbf{I}=f^{\mathcal{C}}\left(\left[\mathbf{I}_{r g b}, \mathbf{I}_h\right]\right)
$$



其中$\mathcal{A}^f=\left\{\mathcal{A}_i^f\right\}_{i=1}^n$代表最后所有点的高斯属性，$\bf I_{rgb}$是最终渲染出来的RGB图像，$\bf I_{h}$是用于CNN refinement的潜在特征图像。

与[之前工作]([YuelangX/Gaussian-Head-Avatar: [CVPR 2024\] Official repository for "Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians"](https://github.com/YuelangX/Gaussian-Head-Avatar))不同的是,我们不使用CNN来进行超分，而是保持输入和输出具有相同的分辨率，以进行细化；我们的目标是使用大规模的训练数据，使CNN能够捕捉到泛化的结构化外观先验知识，这对于3DGS-based表征方法是难以做到的，后面的实验分析也证明了，只用CNN来细化，得到的效果确实更加逼真，实现了few-shot的个性化操作。

* **Overall Representation**

至此，整个的头部化身被正式地定义为：


$$
\mathcal{H}:\left(\mathcal{M} ; f^{\mathcal{M}}, f^{\mathcal{C}}, \mathbf{f}, \mathbf{z}\right) \mapsto \mathbf{I}
$$



### Head Prior Learning

我们高度依赖于头部的先验知识，以只有几个输入图像情况下无约束地实现高保真的化身重建。在各种先验知识中，我们的目标是从能获得的具有多个身份的多视图动态头部数据中学习高质量、可动画化和3D连续的头部先验知识。因此，先验学习阶段的目标是去使用GAPNet用id码$\mathbf{Z}_{1 \ldots k}$和其他优化的网络参数来学习对应的$k$个人头化身，在开始模型训练之前，我们首先对训练数据进行FLAME估计得到$\text{M}$。然后，我们使用这些数据来联合优化$\mathcal{M}, f^{\mathcal{M}},$$f^{\mathcal{C}},\mathrm{f},$和$\bf z$。这是总的损失函数：


$$
\mathcal{L}=\mathcal{L}_{rec}(\mathbf{I},\mathbf{I}^{*})+\mathcal{L}_{rec}(\mathbf{I}_{rgb},\mathbf{I}^{*})+\lambda_{m}\mathcal{L}_{rec}(\mathbf{I}_{m},\mathbf{I}_{m}^{*})+\mathcal{L}_{reg},
$$


其中$\mathcal{L}_{rec}$和$\mathcal{L}_{reg}$分别表示图像重建损失和训练正则化损失,真值图被表示为$\mathbf{I}^{*}$,为了提高嘴部区域的保真度，我们受[FlashAvatar]([USTC3DV/FlashAvatar-code: [CVPR 2024\] The official repo for FlashAvatar](https://github.com/USTC3DV/FlashAvatar-code))启发,进一步用的掩蔽的真实嘴部区域$\mathbf{I}_{m}^{*}$来监督掩蔽的渲染嘴部区域$\mathbf{I}_{m}$;具体来说，图像的重建损失为：


$$
\mathcal{L}_{rec}=\lambda_{l1}\mathcal{L}_{l1}+\lambda_{ssim}\mathcal{L}_{ssim}+\lambda_{lpips}\mathcal{L}_{lpips}
$$


与此同时，训练正则化损失为：


$$
\mathcal{L}_{reg}=\lambda_{\alpha}\mathcal{L}_{\alpha}+\lambda_{s}\mathcal{L}_{s}+\lambda_{\mu}\mathcal{L}_{\mu}+\lambda_{arap}\mathcal{L}_{arap},
$$


这其中包括了不透明度正则项$\mathcal{L}_{\alpha}=\|\mathbf{I}_{\alpha}\mathbf{-}\tilde{\mathbf{I}}_{mask}\|_{1}$,原始局部scale正则化$\mathcal{L}_s=\|\max(\mathbf{s},\epsilon_s)\|_2$,原始局部位置正则化$\mathcal{L}_\mu=\|\max(\boldsymbol{\mu},\epsilon_\mu)\|_2$,还有来自[变形领域经典论文]([igl.ethz.ch/projects/ARAP/arap_web.pdf](https://igl.ethz.ch/projects/ARAP/arap_web.pdf))(~~论文我也没看过~~)的ARAP损失$\mathcal{L}_{arap}$，....

### Few-shot Personalization

在先验学习阶段之后，我们用GAPNet来编码动态头部先验知识;因此所有GAPNet学到的参数都可以作为强大的先验用于few-shot甚至one-shot的个性化人头重建。

在个性化重建之前，我们采用了一个**tracker**(~~这是啥东西，这个要学一下~~)获取输入图像的FLAME参数。给定具FLAME跟踪的输入图像，我们首先通过inversion从id_codebook中找到最相似的化身。具体地说，我们优化部件特定的线性组合权重$\textbf{w}\in\mathbb{R}^{\tilde{k}\times p\times1}$，以获得用于渲染与输入相似的化身的身份码$\mathbf{z}^{*} = \mathrm{softmax}(\mathbf{w})\odot\mathbf{z} \in \mathbb{R}^{k\times p\times c_{2}}$。在反演优化过程中，除了$\mathbf{w}$之外，我们保持网络的所有参数冻结。在形式上，给定目标身份的输入图像$\bf I ^{*}$，我们优化以呈现类似于目标身份的图像$\bf I$。

然后，我们开始微调以更新网络的参数，以便化身可以从输入中捕获目标身份的细节。我们通过**三种策略**在这个过程中利用先验知识。首先，我们对除**特征编码$\bf f$之外**的所有参数使用小的学习率。接下来，我们通过排除嘴部区域的微调过程来充分利用之前提取的Part-based的先验，因为用很少的输入来建模高度灵活的嘴部区域是十分有挑战性的。最后，我们应用视图正则化来防止目标视图的过拟合：具体地，我们将具有中性面$\{\mathbf{R}_i\}_{i=1}^m$的一些参考视图的微调结果约束为接近微调$\{\tilde{\mathbf{R}}_i\}_{i=1}^m$之前的渲染结果，其中$m$是生成的参考视图的数量。利用先验知识，我们的个性化化身实现了稳定的再现，同时保留了目标身份的细节。微调是通过最小化方程中的损失函数来进行的:


$$
\arg\min_\xi\mathcal{L}_f=\mathcal{L}(\mathbf{I},\mathbf{I}^*)+\lambda_{ref}\sum_{i=1}^m(\mathcal{L}(\mathbf{R}_i,\tilde{\mathbf{R}}_i)),
$$

其中$\xi$表示所有可学习的参数，$\lambda_{ref}$用于平衡不同的损失项。



> ```
> @article{zheng2024headgap,
>   title={HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors},
>   author={Zheng, Xiaozheng and Wen, Chao and Li, Zhaohu and Zhang, Weiyi and Su, Zhuo and Chang, Xu and Zhao, Yang and Lv, Zheng and Zhang, Xiaoyuan and Zhang, Yongjie and Wang, Guidong and Xu Lan},
>   journal={arXiv preprint arXiv:2408.06019},
>   year={2024}
> }
>     
> ```

