---
title: SIGGRAPH 2024 URAvatar

date: 2024-10-30 17:00:00 +0800

categories: [3D computer vision, Face, papers]

tags: [3D computer vision, Face, papers]

math: true

mermaid: true

description: URAvatar
---

# Learn2Talk: 3D Talking Face Learns from 2D Talking Face



### Abstract

the aspect of lip- synchronization (lip-sync) and speech perception 不如2D

语音驱动的人脸动画方法通常分为3D和2D两大类，这两类方法近年来都引起了相当大的研究兴趣。然而，据我们所知，3D语音人脸在口型同步和语音感知方面的研究并不像2D语音人脸那样深入。考虑到这两个子领域之间的差距，我们提出了一个名为Learn2Talk的学习框架，该框架可以利用2D Talk领域的两个特长来构建更好的3D Talk网络。首先，受音视频同步网络的启发，我们设计了一种3D同步唇部专家模型，以实现音频和3D人脸运动之间的**唇部同步**。其次，使用从2D语音人脸方法中选择的**教师模型**来指导音频到3D运动回归网络的训练，以产生更高的3D顶点精度。大量实验表明，与现有技术相比，该框架在口型同步、顶点准确度和语音感知等方面具有优势。最后，我们展示了该框架的两个应用：VSR和基于语音驱动的3DGS的化身动画。本文的项目页面为：https://lkjkjoiuiu.github.io/Learn2Talk/.

### contributions

* ***Framework：***我们为3D说话人脸提出了一个新颖的学习框架，该框架从2D说话人脸方法中学习唇语同步和语音感知能力。据我们所知，这条研究路线从未用以前的方法探索过。 
* ***Lip-sync：***我们成功地将SyncNet从pixels域扩展到3D motion域。提出的SyncNet 3D可以用作训练中的判别器，以增强唇齿同步，并用作测试中的指标，以补充合成3D运动质量的评估。这部分工作解决了一个大家几乎都存在的关键问题：如何明确地建模3D运动和输入信号之间的同步？
* ***Speech Perception：***我们通过lipreading的约束将知识从2D说话面部方法提取到音频到3D运动回归模型。因此，预测的3D面部运动在嘴唇顶部产生更高的准确性，从而引发与相应音频的类似感知。在我们的方法提出之前，唇读约束主要用于视频驱动的3D动画，但很少用于音频驱动的3D动画。
* ***Application：***作为一个应用，我们的方法可用于驱动由3DGS构建的全头部化身，从而实现学术界**第一个可语音驱动的基于3DGS的化身动画。**

### RELATED WORD

***Speech-driven 3D Facial Animation***

该领域的方法可以通过音频驱动预测顶点的位置来动画3D面部模型。据我们所知，该领域的研究工作量相对少于语音驱动的2D面部动画。Karras等人[40]利用LSTM来学习从**音频到3D顶点坐标的映射**，并利用额外的情感来控制head avatar 的情感状态。VOCA 通过使用**身份条件反射**将模型扩展到多个主体，解决特定说话者的特定化问题。在VOCA中，**将提取的音频特征与说话者的one hot vector级联**，然后**对融合的特征进行解码以输出3D顶点位移**而不是顶点坐标。MeshTalk注意到VOCA表现出不可思议或静态的面部动画，提出了一个分类潜在空间来解开音频相关和音频不相关信息，以便可以合成不相关的面部区域的合理运动。GDPnet改进了VOCA，具有更多与几何相关的变形，这是通过添加非线性面部重建表示作为潜在空间的引导来实现的。为了解决与短音频窗口中音素级特征相关的不准确的嘴唇运动问题，FaceFormer [24]提出了一种基于变换器的自回归模型来编码长期音频上下文，并获得了重要的性能提升。为了减少跨模式模糊性，CodeTalker [25]使用VQ-VAE [42]首先通过对真实面部运动进行自重建来学习codebook，然后在学习到的离散运动空间中部署FaceFormer。受扩散模型最新进展的启发，还有一些方法采用扩散模型来捕捉语音和面部运动之间的多对多关系。

***Video-driven 3D Facial Animation***

我们简要回顾了视频驱动的3D面部动画方法：单图像3D人脸重建方法，例如RingNet [44]、Deep 3DFace [45]、DECA [46]、EMOCA和DCT一样，可用于通过**视频逐帧方式**生成3D面部运动。还有一些方法[49]-[53]利用单目面部视频的动态信息来约束对象的面部形状或对面部重建施加**时间一致性**。

### METHOD



#### Preliminaries: Relightable 3D Gaussians





#### Universal Relightable Prior Model

