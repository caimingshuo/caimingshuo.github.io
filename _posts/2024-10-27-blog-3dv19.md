---
title: CVPR 2024 E3Gen

date: 2024-10-27 17:00:00 +0800

categories: [3D computer vision, Face, papers]

tags: [3D computer vision, Face, papers]

math: true

mermaid: true

description: E3Gen
---

# $E^3$Gen: Efficient, Expressive and Editable Avatars Generation

### Abstract



### contributions

* 
* 
* 

### PRELIMINARY

#### SMPL-X

SMPL-X 是一种可动画化的参数化人体模型，用参数化可变形网格$M(\beta,\theta,\psi)$表示人体（无衣服）。该模型由 10,475 个顶点和 54 个关节组成，可以控制手势和面部表情。 变形过程可以表述如下：
$$
M(\beta,\theta,\psi)=LBS(T_P(\beta,\theta,\psi)),J(\beta),\theta,W),
$$
其中$\beta,\theta,\psi$分别代表形状，姿势和表情参数。线性混合蒙皮函数$𝐿𝐵𝑆(·)$，用于根据蒙皮权重$\cal W$和关节位置$J(\beta)$将规范模板$T_{p}$转换为给定姿势 𝜃。规范模板$T_{p}$可以计算为：
$$
T_P(\beta,\theta,\psi)=T_C+B_S(\beta;S)+B_E(\psi;\epsilon)+B_P(\theta;\mathcal{P})
$$
其中,$T_{C}$表示平均形状模板。$B_S(\beta;S)$，$B_E(\psi;\epsilon)$和$B_P(\theta;\mathcal{P})$表示由混合形状$S$ ,$\cal P$和$\epsilon$及其相应的**形状**、**姿势**和**表情**参数计算得到的每顶点位移。

### METHOD

![image-20241028205700831](/Users/apple/Documents/GitHub/caimingshuo.github.io/imgs/3dv/3dv9/E3Gen_method.png)

#### **Generative UV Features Representation**

从之前在可动画化身重建任务中采用 3D 高斯的工作中获得灵感，我们的目标是引入 3D 高斯作为扩散模型的目标空间。 与专注于单个人体的优化重建任务不同，我们必须在多个人体之间进行3D高斯训练，因此需要人体之间的泛化的共享结构。 为了与扩散模型中基于 2DCNN 的去噪网络兼容，共享结构预计为 2D 表示。 因此，我们**建议在 SMPL-X 参数模型定义的 2DUV空间**中去表示基于3D高斯的数字化身。





#### Editing

我们新颖的表示法(生成式UV特征平面)可实现各种个性化的应用效果，包括局部区域编辑和人体之间的属性转移。通过几何和外观的解耦，我们扩展了编辑功能，允许单独编辑几何形状或外观，在实验结果中提供了效果展示(其实还是有点粗糙的)：

<img src="/Users/apple/Documents/GitHub/caimingshuo.github.io/imgs/3dv/3dv9/E3Gen_Editing.png" alt="image-20241028201603592" style="zoom: 33%;" />

**Local Region Editing:**与之前采用单一表征的方法（其中生成的化身是各种属性都纠缠在一起的统一实体）相反，生成的UV特征平面将3D数字化身表示为松散连接到SMPL-X参数模型的3D高斯球的集合。这种表示可以增强编辑化身的灵活性和自由度。每个高斯球都可以通过优化其相关的几何形状或外观UV特征，或通过直接操纵其属性值来**独立修改**。

.**Attribute Transfer:**受益于各个个体之间的结构共享，我们可以通过交换相应的特征来轻松地将几何和外观属性从一个生成的对象转移到另一个对象。UV空间的利用(确保了语义一致性)进一步促进了鼻子、衣服和面部等特定区域的转移。



