---
title: Gaussian Head Avatar

date: 2024-07-24 17:00:00 +0800

categories: [3D computer vision, 3DGS]

tags: [3D computer vision, 3DGS]

math: true

mermaid: true

description: 一篇最新的基于gaussian的人脸工作，正好拿他来入门下人头人脸

---

![image-20240825171438700](imgs/3dv/3dv5/image-2.png)

## 论文摘要

创建高保真的三维数字头像一直是热点，但在**轻量级稀疏视图设置**下仍然存在巨大挑战。在本文中，我们提出了一种由**可控**的3D高斯表示的高斯头部化身，用于高保真头部化身建模。我们优化了**中性3D高斯**和一个**完全学习的基于多层感知器（MLP）的变形场**，以捕捉复杂的表情。这两个部分相互促进，因此我们的方法能够在确保表情准确性的同时，对动态细节进行精细建模。此外，我们设计了**一种基于Signed Distance function**（SDF）和**Deep Marching Tetrahedra**的设计的几何引导初始化策略，以提高训练过程的稳定性和收敛性。实验表明，我们的方法在2K分辨率下即使在夸张表情下也能实现超高清渲染质量，超越了其他最先进的稀疏视图方法。

## 前置知识补充

> 作为一个初入门3dv(尤其是人头人脸)的小白，这篇来自lyb老师团队的工作可以很好的让我去了解看下人头人脸方向较新的进展。

### "中性"的3DGS？

一组在**中性表情**（即没有明显表情变化时的面部状态）下定义的3D高斯球：这些高斯球用于构建人脸头像的基础几何形状，并且它们是头像模型的一部分，可以进一步通过学习到的**变形场**（deformation field）来模拟不同的表情变化。

### SDF知识补充

Signed Distance function，本质还是一个函数，其是为了确定一个点到某个区域边界的距离，然后(顺便)把这个距离定义成一个符号：**点在这个区域内部距离就是正的；点在这个区域的外部，距离就是负的，而位于边界时候，距离自然就是0**，（不要想的太复杂~

> 所以SDF实际的具体应用又是什么呢？

### Deep Marching Tetrahedra

这是一篇英伟达2021年nips的文章，实现了从粗糙的coarse voxels(可以理解为minecraft中的每个方块)进行合成高分辨率的效果。

![image-20240825151347699](imgs/3dv/3dv5/image-1.png)

### linear blend skinning

 这也是图形学比较重要的一个概念，可以想象在一个3D动画小人的运动过程中，我们需要怎样去控制这个小人的运动情况呢：对于刚体(骨骼)的部分，我们可以通过设置关节点来控制其移动；而对于非刚体的部分，例如柔软的皮肤表面(蒙皮)等，linear blend skinning就是一种比较常见的做法了，比较核心的一个公式是：


$$
\overline{\mathbf{v}}_i=\sum_{j=1}^mw_{i,j}\mathbf{T}_j\mathbf{v}_i=\left(\sum_{j=1}^mw_{i,j}\mathbf{T}_j\right)\mathbf{v}_i=\sum_{j=1}^mw_{i,j}\overline{\mathbf{T}}_j^0\left(\mathbf{T}_j^0\right)^{-1}\mathbf{v}_i
$$


这里的$\mathbf{v}_i$表示的是在初始位置下，每个顶点在世界坐标系中的坐标,而$\overline{\mathbf{v}}_i$则指的是前$j$个关节运动之后的运动导致的新位置的叠加;$\mathbf{T}_j^0$表示的是在初始位置下,第$j$个骨骼上的坐标系相对于世界坐标系的变换,而$\overline{\mathbf{T}}_j^0$则指的是在已经运动的状态下,第$j$个骨骼上的坐标系相对于世界坐标系的变换;那么对于$\left(\mathbf{T}_j^0\right)^{-1}\mathbf{v}_i$则是已经将$\mathbf{v}_i$表示的世界坐标系的情况转化到了第$j$个骨骼的局部坐标系上的情况,而后面再去乘以一个$\overline{\mathbf{T}}_j^0$则是把骨骼运动的坐标变换再变回到世界坐标系中.

## 贡献总结

* 提出了如何使用可控的动态的3d高斯去模拟人类的头部头像
* 采用了完全学习的变形场,来精确模拟复杂和夸张的面部表情
* 初始化时候,采用隐式表示来初始化几何和变形,从而在训练的时候可以实现高效,稳健的收敛

## The Representation of Avatar

一般一个静态的3D高斯点,通常需要这些参数:位置$X$(通常是三个坐标),颜色$C$,旋转角度$Q$,大小$S$和透明度$A$,然后我们把高斯函数光栅化并渲染为图像$I$的过程可以记为$I=\mathcal{R}(X,C,Q,S,A;\mu)$,其中$\mu$是相机的参数.

对于如何重建动态图像的问题,作者选择先去构建一个相对基础的,完全可优化的$中性$高斯模型$\{\boldsymbol{X}_0,\boldsymbol{F}_0,\boldsymbol{Q}_0,\boldsymbol{S}_0,\boldsymbol{A}_0\}$,其中$\boldsymbol{X}_0$是指$中性$的高斯在空间中的位置, $\boldsymbol{F}_0$是point-wise feature vector; $\boldsymbol{Q}_0,\boldsymbol{S}_0,\boldsymbol{A}_0$则分贝表示旋转,缩放和不透明度,关于比较重要的颜色属性,则是从$\boldsymbol{F}_0$中进行预测得到相关的动态颜色.


$$
\{X,C,Q,S,A\}=\boldsymbol{\Phi}(\boldsymbol{X}_0,\boldsymbol{F}_0,\boldsymbol{Q}_0,\boldsymbol{S}_0,\boldsymbol{A}_0;\theta,\beta)
$$


其中$\boldsymbol{\Phi}$是一个基于MLP的动态生成器,$\theta,\beta$分别是表情系数和头部姿势。

#### 高斯球的位置$X'$

对于因表情和头部姿势所引起的位移，则是选择使用两个不同的神经网络来进行预测得到；然后将这个位移加入到中性位置中：


$$
\begin{aligned}X'&=\boldsymbol X_0+\lambda_{exp}(\boldsymbol X_0)\boldsymbol f_{def}^{exp}(\boldsymbol X_0,\theta)\\&+\lambda_{pose}(\boldsymbol X_0)\boldsymbol f_{def}^{pose}(\boldsymbol X_0,\beta)\end{aligned}
$$


其中的$\lambda_{exp}()$和$\lambda_{pose}()$是指当前点受到表情或者头部姿势影响的程度，这里作者认为距离那些人脸上特定的3D标准点（例如眼睛，鼻子，嘴巴等位置）距离近的会受到更多表情系数的影响，而更少受到头部姿势的影响；而关于标准点$P_{0}$，是一开始通过3DMM模型进行初始化，然后后面再不断优化得到,对于每一个高斯点的位置$x \in \mathbf{X}_0$，有这样的计算方式


$$
\left.\lambda_{exp}(x)=\left\{\begin{array}{ll}1,&dist(x,\boldsymbol{P}_0)<t_1\\\frac{t_2-dist(x,\boldsymbol{P}_0)}{t_2-t_1},&dist(x,\boldsymbol{P}_0)\in[t_1,t_2]\\0,&dist(x,\boldsymbol{P}_0)>t_2\end{array}\right.\right.
$$


其中$t_{1}$和$t_{2}$是超参数，然后每个位置上位姿$\lambda_{pose}(x)$的影响则是：


$$
\lambda_{pose}(x) = 1-\lambda_{exp}(x)
$$


#### 高斯球的颜色$C'$

与上面定义颜色位置$X'$的不同，颜色属性$C$没有所谓的初始化值(比如上面的$\boldsymbol{X}_0$那样)，所以这里整体的颜色是从两个颜色MLP中直接推理出来,同样的；同样，表情和姿势对于也是随着参考点距离的远近有着不同程度的影响：


$$
C^{\prime}=\lambda_{exp}(\boldsymbol{X}_0)\boldsymbol{f}_{col}^{exp}(\boldsymbol{F}_0,\theta)+\lambda_{pose}(\boldsymbol{X}_0)\boldsymbol{f}_{col}^{pose}(\boldsymbol{F}_0,\beta).
$$


#### 旋转，大小和密度$\{Q',S',A'\}$

这些也是比较常见的高斯球的属性，与$X'$类似，也是通过MLP推理从参考值中推理出来其偏移值


$$
\{Q^{\prime},S^{\prime},A^{\prime}\}=\{\mathbf{Q}_0,\mathbf{S}_0,\mathbf{A}_0\}\\+\lambda_{exp}(\boldsymbol{X}_0)\boldsymbol{f}_{att}^{exp}(\boldsymbol{F}_0,\theta)\\+\lambda_{pose}(\boldsymbol{X}_0)\boldsymbol{f}_{att}^{pose}(\boldsymbol{F}_0,\beta).
$$


之后，与3DGS原版中的设置类似，对高斯球的$Q$和$X$，也即旋转和平移应用规范空间坐标系到世界坐标系的转换矩阵$T$，最后经过渲染得到我们想要的三维人头

## Code & Training in head avater

上文中简单聊了模型，**我们发现好像与原版3DGS的区别并不大？**，下面我们结合具体的代码来分析下整个项目,可以看出其代码十分地清晰:

```bash
│  environment.yaml
│  LICENSE
│  README.md
│  reenactment.py
│  train_gaussianhead.py
│  train_meshhead.py
│
├─.idea
│  │  .gitignore
│  │  Gaussian-Head-Avatar.iml
│  │  misc.xml
│  │  modules.xml
│  │  vcs.xml
│  │  workspace.xml
│  │
│  └─inspectionProfiles
│          profiles_settings.xml
│          Project_Default.xml
│
├─config
│      config.py
│      reenactment_N031.yaml
│      train_gaussianhead_N031.yaml
│      train_meshhead_N031.yaml
│
├─imgs
│      teaser.jpg
│
├─lib
│  ├─apps
│  │      Reenactment.py
│  │
│  ├─dataset
│  │      DataLoaderX.py
│  │      Dataset.py
│  │
│  ├─module
│  │      CameraModule.py
│  │      GaussianHeadModule.py
│  │      MeshHeadModule.py
│  │      SuperResolutionModule.py
│  │
│  ├─network
│  │      MLP.py
│  │      PositionalEmbedding.py
│  │      Upsampler.py
│  │
│  ├─recorder
│  │      Recorder.py
│  │
│  ├─trainer
│  │      GaussianHeadTrainer.py
│  │      MeshHeadTrainer.py
│  │
│  └─utils
│          dmtet_utils.py
│          general_utils.py
│          graphics_utils.py
│
└─preprocess
        preprocess_nersemble.py
        remove_background_nersemble.py
```

而且作者很贴心的把代码包装成了三个部分，分别是两阶段的训练`train_meshhead.py`和`train_gaussianhead.py`，他们分别用于训练mesh和gaussian的头部建模；然后是用于用于展示重建效果的`reenactment.py`文件，我们来逐个拆解这些文件。

### Training of Meshhead

`train_meshhead.py`这个文件写的十分地清晰，

## 参考资料

> [Linear blend skinning(LBS)线性混合蒙皮笔记 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/693202505)

