---
title: Triplane/EG3D

date: 2024-08-19 17:00:00 +0800

categories: [3D computer vision, Face]

tags: [3D computer vision, Face]

math: true

mermaid: true

description: 

---

这个故事要从StyleGAN开始讲起。

## StyleGAN系列

StyleGAN解决的是PGGAN特征纠缠的问题，而PGGAN则是解决了GAN在

## EG3D&Triplane

<img src="../imgs/3dv/3dv7/image-2.png" alt="image-20240828200647041" style="zoom: 50%;" />

> NeRF(a)的做法是使用带有位置编码的全连接层来表征一个场景，这样**全隐式**带来的问题就是慢。
>
> 体素Voxels(b)的方式就比较直接，但是很难实现高分辨率的渲染。

如图，**Tri-plane已经成为一种新的表示范式**，其之所以可以被称之为是hybrid的，是因为其将图片**显式**的特征归结到三个相互正交的平面上，每个特征平面的size都是$N\times N\times C$,因此，我们可以很方便地将一个三维空间的中的点分别投影到这三个特征平面上，从而得到特征向量$F_{XY},F_{XZ},F_{YZ}$,(双线性插值),然后直接将$\mathbf{F}=F_{XY}+F_{XZ}+F_{YZ}$得到聚合的3D特征向量，然后我们再使用一个小的MLP（**隐式**），直接就把$\mathbf{F}$输出为了颜色和密度.

> “我们的3D GAN框架由这样几个部分组成：一个位姿条件的基于StyleGAN2的**特征**生成器和映射网络，带有轻量特征解码器的tri-plane 3D表征，一个神经体渲染器，一个超分模块，一个位姿条件的StyleGAN的判别器(**dual discrimination**)”。

所以EG3D自然可以算是[StyleGAN2]([NVlabs/stylegan2: StyleGAN2 - Official TensorFlow Implementation (github.com)](https://github.com/NVlabs/stylegan2))的直接改进,在上面的piplane中，我们NeuralRender本身和其左边的东西都可以算是新的"Generator"，在阅读论文时要与StyleGAN2中"Generator"的概念分隔开。

## SSO Experiment

与GAN要求的泛化性质不同，single-scene over fitting experiment（SSO）是直接从多个输入视图直接提取特征，不需要复杂的特征生成，直白点就是不需要StyleGAN2的生成器来生成tri-prlane的特征，这样使得模型可以专注于对特定场景的学习。       

在GAN设置中，这里的神经渲染器不是生成 RGB 图像，而是汇总32通道三平面中每个通道的特征，并根据给定的相机姿势预测出32通道(32张)特征图像。                                                                                                                                                                                                             

## 效果运行

为了明白我们在做的是什么事情，因此我们先选择进行一边推理的过程，观察下效果

```bash
(eg3d) root@autodl-container-b63c498021-4a56fd7f:~/eg3d/eg3d# python gen_videos.py --outdir=out --trunc=0.7 --seeds=0-3 --grid=2x2 --network datatpoint/ffhq512-128.pkl
/root/miniconda3/envs/eg3d/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
Loading networks from "datatpoint/ffhq512-128.pkl"...
Setting up PyTorch plugin "bias_act_plugin"... Done.
Setting up PyTorch plugin "upfirdn2d_plugin"... Done.
100%|█████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:41<00:00,  2.87it/s]
```

在推理阶段，在autodl上组的2080ti的卡在一分钟内就完成了生成视频的效果，这里我们选择使用预置的ffhq512-128.pkl权重进行下载，最终得到的效果如下：

<video src="/Users/apple/Downloainterpolation.mp4"></video>

## 代码结构

EG3D的代码结构比较清晰，但是其中的细节部分对于我这样的入门菜🐔还是相当困难，下面来逐步分解一下代码：

### Train.py

* CLICK

整个代码文件中最引人注目的是`train.py`，而在`train.py`中最抓人眼球的一个是`launch_training函数`,另一个则是click定义的一大堆参数设置：

```python
@click.command()

# Required.
@click.option('--outdir',       help='Where to save the results', metavar='DIR',                required=True)......
# Optional features.
@click.option('--cond',         help='Train conditional model', metavar='BOOL',                 type=bool, default=True, show_default=True)......

# Misc hyperparameters.
@click.option('--p',            help='Probability for --aug=fixed', metavar='FLOAT',            type=click.FloatRange(min=0, max=1), default=0.2, show_default=True)......

# Misc settings.
@click.option('--desc',         help='String to include in result dir name', metavar='STR',     type=str)
@click.option('--metrics',      help='Quality metrics', metavar='[NAME|A,B,C|none]',            type=parse_comma_separated_list, default='fid50k_full', show_default=True)......
# 
@click.option('--sr_module',    help='Superresolution module', metavar='STR',  type=str, required=True)
@click.option('--neural_rendering_resolution_initial', help='Resolution to render at', metavar='INT',  type=click.IntRange(min=1), default=64, required=False)......

```

我们可以观察出，通过`@click.command()`装饰器将下面的函数装饰为命令，而通过`@click.option()`实现对于函数中各种参数的定义，其中的规则如下:

```bash
default: 设置命令行参数的默认值

help: 参数说明

type: 参数类型，可以是 string, int, float 等

prompt: 当在命令行中没有输入相应的参数时，会根据 prompt 提示用户输入

nargs: 指定命令行参数接收的值的个数

metavar：如何在帮助页面表示值
```

相比起argparse，click方式的优势似乎是能针对某个特定的函数来设置参以及配合上`dnnlib.EasyDict()`方法，使得这些函数的传入在层层的子字典嵌套中更为方便和规律一些。

* LANUCH TRANING函数

该函数先是设置好输出目录，然后打印一下一些比较重要的参数，是否空运行，以及设置好输出的目录，真正的启动训练则是下面这几句：

```python
    print('Launching processes...')
    torch.multiprocessing.set_start_method('spawn')
    with tempfile.TemporaryDirectory() as temp_dir:
        if c.num_gpus == 1:
            subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
        else:
            torch.multiprocessing.spawn(fn=subprocess_fn, args=(c, temp_dir), nprocs=c.num_gpus)
```

`torch.multiprocessing.set_start_method`是在设置`torch`中内置的几种方法，其中`spawn`是比较常见的一种，随后的`with tempfile.TemporaryDirectory() as temp_dir`为后面的多卡的设置提供了临时的文件目录，不过这里`with ... as ...`的直接读取文件的方式平时可以多用一下，如果是多卡，则是直接通过`spawn`方式直接完成，如果只有单卡，则会转到`subprocess_fn`函数中：	

```python
def subprocess_fn(rank, c, temp_dir):
    dnnlib.util.Logger(file_name=os.path.join(c.run_dir, 'log.txt'), file_mode='a', should_flush=True)

    # Init torch.distributed.
    if c.num_gpus > 1:
        init_file = os.path.abspath(os.path.join(temp_dir, '.torch_distributed_init'))
        if os.name == 'nt':
            init_method = 'file:///' + init_file.replace('\\', '/')
            torch.distributed.init_process_group(backend='gloo', init_method=init_method, rank=rank, world_size=c.num_gpus)
        else:
            init_method = f'file://{init_file}'
            torch.distributed.init_process_group(backend='nccl', init_method=init_method, rank=rank, world_size=c.num_gpus)

    # Init torch_utils.
    sync_device = torch.device('cuda', rank) if c.num_gpus > 1 else None
    training_stats.init_multiprocessing(rank=rank, sync_device=sync_device)
    if rank != 0:
        custom_ops.verbosity = 'none'

    # Execute training loop.
    training_loop.training_loop(rank=rank, **c)
```

首先是`Logger`的设置，在之前的遥感炼丹中，我并没有使用过类似的Logger设置，：





## 参考资料

> 
>

